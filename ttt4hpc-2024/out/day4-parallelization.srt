1
00:00:01,660 --> 00:00:04,380
So should we start with the motivation section now?

2
00:00:04,380 --> 00:00:05,180
Is that where we are?

3
00:00:06,460 --> 00:00:10,060
Yeah, at least get an overview on why are we here.

4
00:00:10,060 --> 00:00:10,940
Yeah.

5
00:00:10,940 --> 00:00:14,300
Also, interesting notion that [name] brought up,

6
00:00:14,300 --> 00:00:16,940
that should we parallelize?

7
00:00:16,940 --> 00:00:19,420
Because I hadn't thought about it.

8
00:00:19,740 --> 00:00:27,940
Like, I have taken it as a given that we parallelize.

9
00:00:27,940 --> 00:00:29,300
Good question.

10
00:00:29,300 --> 00:00:37,660
And this lesson, the other name of it is called Parallelizing Without Parallelizing.

11
00:00:37,660 --> 00:00:43,620
So what does that mean?

12
00:00:43,620 --> 00:00:48,060
And is that sort of what you said about not wanting to parallelize?

13
00:00:48,060 --> 00:01:03,080
Yeah. Well, if I jump in here, the parallelizing without parallelizing means that we are not

14
00:01:03,080 --> 00:01:14,720
doing parallelization within a language. So, if I have a Python script, I'm not using,

15
00:01:14,720 --> 00:01:25,800
For example, Python's multi-processing, multi-threading properties.

16
00:01:25,800 --> 00:01:32,720
So that means we're basically running the same thing multiple times, but each individual

17
00:01:32,720 --> 00:01:36,160
one doesn't have any special parallelization.

18
00:01:36,160 --> 00:01:37,160
Exactly.

19
00:01:37,160 --> 00:01:38,160
Okay.

20
00:01:38,160 --> 00:01:39,160
Yeah.

21
00:01:39,160 --> 00:01:47,960
That sounds like a pretty good deal overall, I mean, if you can get parallelization without

22
00:01:47,960 --> 00:01:51,040
having to do it.

23
00:01:51,040 --> 00:02:01,360
I think it's a fantastic deal, because you don't have to go into the intricacies of the

24
00:02:01,360 --> 00:02:09,080
language, and what you have to do is you have to learn this kind of meta layer of the HPC.

25
00:02:09,080 --> 00:02:14,360
how do I efficiently submit basically the same job

26
00:02:14,360 --> 00:02:17,800
with a little bit different parameters,

27
00:02:17,800 --> 00:02:19,160
like multiple times.

28
00:02:22,200 --> 00:02:35,720
So something changed here.

29
00:02:35,720 --> 00:02:37,120
Yeah, OK.

30
00:02:37,120 --> 00:02:42,480
And it's this sort of what I've heard,

31
00:02:42,480 --> 00:02:46,160
like I've got the idea that in a lot of research,

32
00:02:46,160 --> 00:02:47,600
this works better.

33
00:02:47,600 --> 00:02:51,160
Like for example, when our team is buying GPUs,

34
00:02:51,160 --> 00:02:56,040
we can spend extra for these GPUs with fast communication.

35
00:02:56,040 --> 00:03:00,320
But the general feeling is, well,

36
00:03:00,320 --> 00:03:04,600
people are more likely to just run multiple jobs at once.

37
00:03:04,600 --> 00:03:11,720
Like if you need to run 10 GPU simulations,

38
00:03:11,720 --> 00:03:18,640
and you can run, let's see, like 10 in one hour,

39
00:03:18,640 --> 00:03:21,320
or 10 at the same time, each taking 10 hours,

40
00:03:21,320 --> 00:03:23,040
it gets done at the same time.

41
00:03:23,040 --> 00:03:28,440
So is that sort of how it is?

42
00:03:28,440 --> 00:03:32,360
Yeah, and also from researchers' point of view,

43
00:03:32,360 --> 00:03:38,760
learning to use this very low-level parallelization. The learning overhead is

44
00:03:41,240 --> 00:03:49,720
something completely different than learning how to do this, well, the embarrassingly parallel one.

45
00:03:49,720 --> 00:03:54,440
Yeah. So there's a lot of text here. I guess we can...

46
00:03:54,440 --> 00:04:02,280
Yeah, and it's a good text, but I recommend reading it on your own pace.

47
00:04:02,920 --> 00:04:06,920
Yeah. So what does this figure here mean?

48
00:04:10,120 --> 00:04:12,760
1 to 10, do something in parallel, or...

49
00:04:15,800 --> 00:04:23,080
Is this saying that if you control your parallelization, you can do 10 things at once,

50
00:04:23,080 --> 00:04:36,080
But if you do 10 things at once, and each of them are trying to use 10 processors, then you've oversubscribed your...

51
00:04:36,080 --> 00:04:38,080
Yeah, what does this mean?

52
00:04:38,080 --> 00:04:49,560
Yeah, I think that's exactly what it means. So giving in first picture, we are doing 10,

53
00:04:49,560 --> 00:04:58,560
like submitting 10 jobs to 10 CPUs, which is very nice from our point of view. It's

54
00:04:58,560 --> 00:05:05,040
very understandable what's happening. And it's very nice from the point of view of the

55
00:05:05,040 --> 00:05:18,240
hardware because it doesn't have to tangle itself in scheduling. But then I think the

56
00:05:18,240 --> 00:05:38,000
The latter picture is, if we also try to do something in parallel, like, in, in the, yeah,

57
00:05:38,000 --> 00:05:40,280
I don't even know how to describe it.

58
00:05:40,280 --> 00:05:41,280
And that's the problem.

59
00:05:41,280 --> 00:05:48,800
I remember this. So sometimes on our cluster, people submit jobs, and they might submit

60
00:05:48,800 --> 00:05:54,000
a bunch of jobs, but they don't configure the job. So each job is trying to use every

61
00:05:54,000 --> 00:06:02,040
CPU on the computing node. And that means that you're running way more than can fit

62
00:06:02,040 --> 00:06:10,000
on there, and it slows down a lot. So I think it's something like that.

63
00:06:10,000 --> 00:06:22,780
So each job should be isolated in a sense that they don't try to use the same CPUs.

64
00:06:22,780 --> 00:06:29,000
So what does it mean to move the parallelization to the scheduler level?

65
00:06:29,000 --> 00:06:31,460
So I guess this is what you've been saying.

66
00:06:31,460 --> 00:06:34,740
So the scheduler is SLURM?

67
00:06:34,740 --> 00:06:37,640
Yes.

68
00:06:37,640 --> 00:06:45,240
on queuing system. Yeah, so basically you tell the scheduler to start many jobs and then

69
00:06:45,880 --> 00:06:50,200
slurm does that and each job is not parallel.

70
00:06:54,040 --> 00:07:01,560
Yeah, okay. There's a good question in the notes. Sometimes it feels like my laptop

71
00:07:01,560 --> 00:07:06,680
multiprocessors are faster than the same amount of processors on HPC. Why is my laptop faster?

72
00:07:07,640 --> 00:07:15,880
And that's a good question. I guess I can answer that from our cluster's hardware level.

73
00:07:16,520 --> 00:07:23,080
So, I mean, the easy answer is that we don't promise it to be faster. So the cluster is

74
00:07:23,080 --> 00:07:30,360
optimized for many processors that have reasonable price that can be run in parallel.

75
00:07:30,360 --> 00:07:39,840
So, yeah, I mean, on average, your laptop is about more or less the single processor

76
00:07:39,840 --> 00:07:42,000
power as the cluster.

77
00:07:42,000 --> 00:07:47,520
The difference is that the cluster has tens of thousands or more of processors, and your

78
00:07:47,520 --> 00:07:53,880
laptop has maybe four to eight, something like that.

79
00:07:53,880 --> 00:08:00,900
But also for our particular cluster at our university, we keep the hardware as long as

80
00:08:00,900 --> 00:08:06,400
it makes sense from the energy efficiency point of view, or until it starts literally

81
00:08:06,400 --> 00:08:11,600
falling apart and becoming so unreliable it doesn't work anymore.

82
00:08:11,600 --> 00:08:16,280
So that means that you might be running something on the cluster that's on one of the older

83
00:08:16,280 --> 00:08:22,320
processors that might be five or more years old, while your laptop is probably not quite

84
00:08:22,320 --> 00:08:23,320
that old.

85
00:08:23,320 --> 00:08:27,960
So it might have more optimizations and so on there.

86
00:08:27,960 --> 00:08:31,120
And that's, well, I mean, from our point of view,

87
00:08:31,120 --> 00:08:33,840
we can either remove working hardware

88
00:08:33,840 --> 00:08:36,600
and have to buy more or keep it there.

89
00:08:36,600 --> 00:08:39,720
And it should always be better to keep it there.

90
00:08:39,720 --> 00:08:42,320
And if someone needs newer, they can request newer.

91
00:08:42,320 --> 00:08:44,040
And this is still processing power

92
00:08:44,040 --> 00:08:47,560
that someone didn't have before.

93
00:08:47,560 --> 00:08:53,160
OK, so on a cluster, the point

94
00:08:53,160 --> 00:09:04,840
is that a single CPU might not be like super new and it might not be the kind of the best

95
00:09:04,840 --> 00:09:12,440
of the bunch, but the point is that when you have like average CPUs in the order of tens

96
00:09:12,440 --> 00:09:21,640
of thousands, then you get the kind of the scaling advantage over the laptop.

97
00:09:21,640 --> 00:09:22,640
Yeah.

98
00:09:22,640 --> 00:09:23,640
Yeah.

99
00:09:23,640 --> 00:09:32,320
And I mean, there definitely are clusters that have really high single processor performance.

100
00:09:32,320 --> 00:09:37,400
But anyway, I guess we should move on.

101
00:09:37,400 --> 00:09:41,800
Next is concepts.

102
00:09:41,800 --> 00:09:45,480
So what do I need to know to get started here?

103
00:09:45,480 --> 00:09:46,480
Okay.

104
00:09:46,480 --> 00:09:52,360
So, first of all, yeah, embarrassingly parallel code.

105
00:09:52,360 --> 00:09:53,360
Yeah.

106
00:09:53,360 --> 00:09:57,520
What does that mean?

107
00:09:57,520 --> 00:09:58,520
Yeah.

108
00:09:58,520 --> 00:10:06,240
So this is the most convenient type of parallelization.

109
00:10:06,240 --> 00:10:12,760
And then what does it say here in the text?

110
00:10:12,760 --> 00:10:20,200
It's code that essentially runs the same function for a large variety of input parameters, where

111
00:10:20,200 --> 00:10:25,400
each step does not depend on anything other than the input data.

112
00:10:25,400 --> 00:10:31,440
So basically we have jobs that are very similar.

113
00:10:31,440 --> 00:10:40,720
They take input parameters, which makes them different, and the results don't depend on

114
00:10:40,720 --> 00:10:43,400
the jobs, don't depend on each other.

115
00:10:43,400 --> 00:10:49,760
So here, all of these calculates can be done at the same time, in theory.

116
00:10:49,760 --> 00:10:50,760
Yes.

117
00:10:50,760 --> 00:10:54,680
Or here for the two-dimensional scan, they can all be done separately.

118
00:10:54,680 --> 00:10:58,680
So it's the whole point of today that we're going to split these out so all of these can

119
00:10:58,680 --> 00:11:01,400
be run at the same time using Slurp?

120
00:11:01,400 --> 00:11:02,400
Yes.

121
00:11:02,400 --> 00:11:04,400
Or something like that.

122
00:11:04,400 --> 00:11:05,400
Okay, yeah.

123
00:11:05,400 --> 00:11:12,280
So instead of this being Python code, which is serial, the scheduler does it in parallel.

124
00:11:12,280 --> 00:11:14,280
Yes, exactly.

125
00:11:14,280 --> 00:11:24,360
So we are basically always moving the loop, looping into the photo scheduler.

126
00:11:24,360 --> 00:11:31,960
By the way, the name embarrassingly parallel code or parallel, embarrassingly parallel

127
00:11:31,960 --> 00:11:42,120
is a bit, it's a bit funny to me because it has this negative connotation when in fact

128
00:11:42,120 --> 00:11:48,320
it is like, it's a super good thing that something is embarrassingly parallel.

129
00:11:48,320 --> 00:11:54,560
So like, I would rather call it like happily parallel or something.

130
00:11:54,560 --> 00:11:55,560
Easily parallel.

131
00:11:55,560 --> 00:11:56,560
Yeah.

132
00:11:56,560 --> 00:11:57,560
Yeah.

133
00:11:57,560 --> 00:12:00,440
So, but, but this is like, uh, this is standard jargon.

134
00:12:00,440 --> 00:12:05,860
So that's why we have adopted the embarrassingly parallel.

135
00:12:05,860 --> 00:12:10,700
Is it like embarrassingly parallel because someone gets hired to make something parallel

136
00:12:10,700 --> 00:12:16,020
and then they work on it for a year and come back and say, oh yeah, I just parallelized

137
00:12:16,020 --> 00:12:19,540
it in the scheduler and didn't modify the code.

138
00:12:19,540 --> 00:12:20,540
Yeah.

139
00:12:20,540 --> 00:12:23,140
They're like, yeah, something.

140
00:12:23,140 --> 00:12:24,140
Something like that.

141
00:12:24,140 --> 00:12:25,140
Yeah.

142
00:12:25,140 --> 00:12:27,420
But it's just a weird word choice.

143
00:12:27,420 --> 00:12:28,420
Yeah.

144
00:12:28,420 --> 00:12:34,620
But I guess the point is we always want to do embarrassing parallel if possible.

145
00:12:34,620 --> 00:12:36,620
Yes.

146
00:12:36,620 --> 00:12:38,620
Okay.

147
00:12:38,620 --> 00:12:42,100
What about size of jobs?

148
00:12:42,100 --> 00:12:54,020
There, I would say that the first sentence basically summarizes that in general, jobs

149
00:12:54,020 --> 00:12:59,260
should not be too short on a cluster.

150
00:12:59,260 --> 00:13:06,060
Because if you have tens of thousands of jobs or thousands of jobs and each take only, let's

151
00:13:06,060 --> 00:13:10,020
say, a minute to run, then there's

152
00:13:10,020 --> 00:13:14,860
going to be a lot of scheduling overhead

153
00:13:14,860 --> 00:13:19,580
because the scheduler, or Slurm in our case,

154
00:13:19,580 --> 00:13:23,340
needs to do all the scheduling and queuing work.

155
00:13:23,340 --> 00:13:32,100
So it is very rough on the cluster itself.

156
00:13:32,100 --> 00:13:38,660
So basically, don't make them too short, and I see there's a question in the notes where

157
00:13:38,660 --> 00:13:47,140
there's some more details there. So should we go on? Okay, yeah. And I think it is pitfalls,

158
00:13:48,100 --> 00:13:57,300
concurrency issues. So there's a lot here. What should I share? I think if we go through the

159
00:13:57,300 --> 00:14:06,100
summary, because again, the text here is, which there is kind of a lot. I would recommend

160
00:14:06,100 --> 00:14:14,660
going through that in people's own pace, but then we can, let's pick some interesting

161
00:14:15,940 --> 00:14:20,500
things from the summary and talk about them. What do I need to know?

162
00:14:20,500 --> 00:14:30,980
Well, in general, what does the concurrency issue mean?

163
00:14:30,980 --> 00:14:42,700
To me, it means that two or more things like codes are trying to access the same resource,

164
00:14:42,700 --> 00:14:49,220
which could be a file or a database.

165
00:14:49,220 --> 00:14:56,660
So do you do you agree with this or yeah like I mean I guess you can ask what needs to be

166
00:14:56,660 --> 00:15:03,080
accessed concurrently and I guess it's things like reading in input data and maybe writing

167
00:15:03,080 --> 00:15:09,760
out data if it's going to the same place I guess the number one rule of a cluster is

168
00:15:09,760 --> 00:15:16,000
don't have a bunch of processes all try to write to the same file exactly unless you're

169
00:15:16,000 --> 00:15:24,640
carefully managing it. So is that the main? Is it bad to read from the same file multiple times?

170
00:15:26,080 --> 00:15:38,560
That's a good question. I think I have Googled this and read the stack overflow

171
00:15:38,560 --> 00:15:46,720
discussion like multiple times during my work years. And I think the answer is like reading is

172
00:15:46,720 --> 00:15:53,040
fine. Yeah. I've never heard of a problem with reading unless it's, you're completely

173
00:15:53,040 --> 00:15:59,520
overloading the system. But at that point, well. Yeah. There's something else also going on there.

174
00:15:59,520 --> 00:16:09,440
if you, by reading a file, you overload the system. So yeah, go ahead, sorry.

175
00:16:09,440 --> 00:16:13,280
But simultaneous writes, so what about that?

176
00:16:14,720 --> 00:16:22,880
Yeah, that is the problem. And like you said, that the main point is don't try to write to the same

177
00:16:22,880 --> 00:16:33,440
file at the same time. And the solution here in the summary is given, make sure that you,

178
00:16:34,160 --> 00:16:41,440
for example, use when you write to a file that the file name has, for example, the job ID

179
00:16:41,440 --> 00:16:51,200
or the parameters in it. So that way you make sure that the results and output files are

180
00:16:51,200 --> 00:16:54,320
written to job-specific files.

181
00:16:54,320 --> 00:16:55,320
Yeah.

182
00:16:55,320 --> 00:17:01,440
Okay, so any time we're doing this embarrassing parallel, if we're saving data somewhere,

183
00:17:01,440 --> 00:17:06,600
make sure it's writing it separately, and then combine it later, I guess?

184
00:17:06,600 --> 00:17:07,600
Yes.

185
00:17:07,600 --> 00:17:08,600
If you need to?

186
00:17:08,600 --> 00:17:09,600
Okay.

187
00:17:09,600 --> 00:17:13,360
Yeah, combine and use a collection script or...

188
00:17:13,360 --> 00:17:15,080
Yeah, okay.

189
00:17:15,080 --> 00:17:19,880
Do you know, is it safe to append to files from multiple jobs?

190
00:17:19,880 --> 00:17:27,720
Like, if you're only appending and not doing other modifications.

191
00:17:27,720 --> 00:17:29,880
I wouldn't.

192
00:17:29,880 --> 00:17:32,440
I guess that's the safe answer.

193
00:17:32,440 --> 00:17:39,880
I mean, I'm wondering if it's safe, but if it's too big, like, maybe we can ask someone

194
00:17:39,880 --> 00:17:41,360
and see.

195
00:17:41,360 --> 00:17:48,880
Yeah, and it's like, it's, I know it may sound weird because when you think about writing

196
00:17:48,880 --> 00:17:55,360
a line of text, for example, in a file that you think of it like an instant instantaneous

197
00:17:57,440 --> 00:18:07,920
thing. But it does take time. And then if you have like multiple scripts writing all the time,

198
00:18:07,920 --> 00:18:16,080
then there will be conflicts. And what you end up is, well, I like to think that you end up with

199
00:18:16,080 --> 00:18:23,120
corrupted files, but I guess it can be also that everything slows down because...

200
00:18:23,120 --> 00:18:30,960
Yeah. From my understanding of Unix, there's certain operations that should be atomic,

201
00:18:30,960 --> 00:18:37,600
meaning it all happens before anything else happens. And I'd like to think that appending

202
00:18:37,600 --> 00:18:44,320
to a file is one of them. So if you append small enough data, it can always be done in one write.

203
00:18:44,320 --> 00:18:50,960
It would work, but I really don't know if once it goes through the network layer and

204
00:18:50,960 --> 00:19:02,200
the scratch file system layer, if that would still be the case, but okay, anyway.

205
00:19:02,200 --> 00:19:05,600
Should we go on or is there more about this database access?

206
00:19:05,600 --> 00:19:13,480
Well, I would think of a database access the same way as the file system, very roughly.

207
00:19:13,480 --> 00:19:21,440
But the right database should handle multiple writes at the same time.

208
00:19:21,440 --> 00:19:23,000
Yeah, exactly.

209
00:19:23,000 --> 00:19:34,680
So that the database will have concurrency, like, not all databases, but...

210
00:19:34,680 --> 00:19:40,520
I've heard of some of our users doing things like setting up a database which all the jobs

211
00:19:40,520 --> 00:19:42,040
would read or write to.

212
00:19:42,040 --> 00:19:49,840
I mean, I guess they can't all do a lot at the same time.

213
00:19:49,840 --> 00:19:54,240
You can't have millions of things writing to there because of the database performance,

214
00:19:54,240 --> 00:19:55,240
but a few things.

215
00:19:55,240 --> 00:19:58,560
That's sort of the point of a database.

216
00:19:58,560 --> 00:20:08,580
Yeah. And like you said, that like a proper database, for example, if we name some databases,

217
00:20:08,580 --> 00:20:19,680
so if you pick SQLite, that doesn't have concurrency. So that will have the same problem as writing

218
00:20:19,680 --> 00:20:29,320
to a file, but if you pick, for example, Postgres, then that will have concurrency handling.

219
00:20:29,320 --> 00:20:37,080
So it is definitely better the situation to use the database.

220
00:20:37,080 --> 00:20:46,200
So you don't get these conflicts, but there is this kind of like the compromises that

221
00:20:46,200 --> 00:20:57,280
You have to set up and admin the database, and also the database will use some resources,

222
00:20:57,280 --> 00:20:59,200
has to run somewhere.

223
00:20:59,200 --> 00:21:04,080
Okay, should we go on then?

224
00:21:04,080 --> 00:21:05,080
Yeah.

225
00:21:05,080 --> 00:21:06,080
Okay.

226
00:21:06,080 --> 00:21:10,940
Pitfalls, hardware, and server limitations.

227
00:21:10,940 --> 00:21:14,040
So I guess let's look at the summary here again.

228
00:21:14,040 --> 00:21:20,920
So what about this?

229
00:21:20,920 --> 00:21:32,000
What do I need to know?

230
00:21:32,000 --> 00:21:33,600
Did my network connection die?

231
00:21:33,600 --> 00:21:34,600
No.

232
00:21:34,600 --> 00:21:35,600
No, no, no.

233
00:21:35,600 --> 00:21:36,600
Okay.

234
00:21:36,600 --> 00:21:37,600
I'm reading the summary as well.

235
00:21:37,600 --> 00:21:38,600
Okay.

236
00:21:38,600 --> 00:21:39,600
So that's why.

237
00:21:39,600 --> 00:21:40,600
Yeah.

238
00:21:40,600 --> 00:21:44,600
I mean, I guess it's saying...

239
00:21:44,600 --> 00:21:48,600
...is caches...

240
00:21:48,600 --> 00:21:52,600
This goes a little bit too low-level

241
00:21:52,600 --> 00:21:56,600
for me. I guess a lot of it makes sense, like, if you have many small files in parallel,

242
00:21:56,600 --> 00:22:00,600
then that becomes a

243
00:22:00,600 --> 00:22:04,600
bottleneck for IOSpeed.

244
00:22:04,600 --> 00:22:08,600
We already talked about the many

245
00:22:08,600 --> 00:22:13,200
small jobs grouping, maybe we'll talk about later.

246
00:22:13,200 --> 00:22:14,200
Yeah.

247
00:22:14,200 --> 00:22:15,200
So,

248
00:22:15,200 --> 00:22:16,200
I guess.

249
00:22:16,200 --> 00:22:17,200
Yeah.

250
00:22:17,200 --> 00:22:31,200
So, how I read it is that if you have slow type disks that will like spinning disks,

251
00:22:31,200 --> 00:22:41,040
they will be slower, and maybe that you should know where you are writing, that what the

252
00:22:42,560 --> 00:22:49,840
disks are being used, and maybe that will help you debug some problems if something comes up,

253
00:22:50,400 --> 00:23:00,320
that, okay, I have something is going wrong, then maybe it could be something like this.

254
00:23:01,200 --> 00:23:02,200
Yeah.

255
00:23:02,200 --> 00:23:03,200
Too slow a disk.

256
00:23:03,200 --> 00:23:04,200
Yeah.

257
00:23:04,200 --> 00:23:05,200
Okay.

258
00:23:05,200 --> 00:23:06,200
Should we go on then?

259
00:23:06,200 --> 00:23:07,200
Yeah.

260
00:23:07,200 --> 00:23:14,200
I was going to say that talk to your favorite cluster administration about this issue.

261
00:23:14,200 --> 00:23:15,200
Yeah.

262
00:23:15,200 --> 00:23:19,960
I guess every cluster is different, so maybe it's good to see what yours is optimized for.

263
00:23:19,960 --> 00:23:25,360
Like I think ours is more optimized for small file access than some of the big ones and

264
00:23:25,360 --> 00:23:26,360
so on.

265
00:23:26,360 --> 00:23:27,360
Exactly.

266
00:23:27,360 --> 00:23:28,360
Okay.

267
00:23:28,360 --> 00:23:38,660
So, let's go on then, and maybe we can take a break and look at the notes here.

268
00:23:38,660 --> 00:23:45,200
So we've got a lot of good questions here.

269
00:23:45,200 --> 00:23:49,840
Some already answered the new questions.

270
00:23:49,840 --> 00:23:53,720
Some of these I wrote here, hoping that other people would answer.

271
00:23:53,720 --> 00:23:57,720
Is there a Linux command to test the IOSPEED of disks?

272
00:23:57,720 --> 00:23:59,720
Hmm...

273
00:23:59,720 --> 00:24:06,720
Actually, I would be really happy for a cheat sheet of all of these Linux performance measuring commands,

274
00:24:06,720 --> 00:24:12,720
like test disk performance, test CPU performance, and so on.

275
00:24:12,720 --> 00:24:18,720
Time definitely does show how much time your program is taking,

276
00:24:18,720 --> 00:24:24,720
making, but not the raw capacity of the disks themselves.

277
00:24:24,720 --> 00:24:25,720
Hmm.

278
00:24:25,720 --> 00:24:26,720
Hmm.

279
00:24:26,720 --> 00:24:27,720
Yeah.

280
00:24:27,720 --> 00:24:28,720
Okay.

281
00:24:28,720 --> 00:24:33,120
But yes, keep them coming.

282
00:24:33,120 --> 00:24:35,080
And should we go on?

283
00:24:35,080 --> 00:24:39,720
Yeah, just a second.

284
00:24:39,720 --> 00:24:44,640
These three here, I...

285
00:24:44,640 --> 00:24:47,880
Is that the database out of queue?

286
00:24:47,880 --> 00:24:52,500
So yeah, so for this one, basically databases are designed for concurrent access.

287
00:24:52,500 --> 00:24:57,100
They have their own internal queues and journaling and stuff like that.

288
00:24:57,100 --> 00:25:03,200
So you can basically send all the rights to it and it holds them until it can do it safely

289
00:25:03,200 --> 00:25:04,920
and in order.

290
00:25:04,920 --> 00:25:05,920
Yeah.

291
00:25:05,920 --> 00:25:16,320
So if it receives two requests to write something like simultaneously, it has an inner process

292
00:25:16,320 --> 00:25:21,520
to make sure that they are first queued and then written safely.

293
00:25:21,520 --> 00:25:22,520
Yeah.

294
00:25:22,520 --> 00:25:26,280
Sorry, I think I just, I just repeated what you said.

295
00:25:26,280 --> 00:25:27,280
Yeah.

296
00:25:27,280 --> 00:25:28,280
Yeah.

297
00:25:28,280 --> 00:25:29,280
That's good.

298
00:25:29,280 --> 00:25:30,280
On here.

299
00:25:30,280 --> 00:25:31,280
Yeah.

300
00:25:31,280 --> 00:25:32,280
Someone pointed out the dd command.

301
00:25:32,280 --> 00:25:35,640
So dd basically, what does it even stand for?

302
00:25:35,640 --> 00:25:37,560
Duplicate data or something like that.

303
00:25:37,560 --> 00:25:43,160
It can do raw reads and writes, not raw, but it basically reads and writes data.

304
00:25:43,160 --> 00:25:50,840
So you can use it to say, how long does it take to read one gigabyte of data, or so on.

305
00:25:50,840 --> 00:25:58,160
But let's let people keep adding in more stuff there.

306
00:25:58,160 --> 00:26:01,440
Okay.

307
00:26:01,440 --> 00:26:07,400
So now I think we get to the main part of the lesson.

308
00:26:07,400 --> 00:26:09,240
Yes.

309
00:26:09,240 --> 00:26:17,080
So what's our example for the rest of the time?

310
00:26:17,080 --> 00:26:22,540
So from what I understand, I'll be typing out a real example, which is partly a mystery

311
00:26:22,540 --> 00:26:23,540
to me.

312
00:26:23,540 --> 00:26:24,540
Okay.

313
00:26:24,540 --> 00:26:25,540
Yeah.

314
00:26:25,540 --> 00:26:43,300
The scenario that we are starting with, it's kind of a reminder of quite a realistic scenario,

315
00:26:43,300 --> 00:26:54,820
which is that people are developing their pipeline or workflow in a Jupyter notebook.

316
00:26:54,820 --> 00:27:07,460
But in order to use the HPC efficiently, we want to take that one notebook and turn it

317
00:27:07,460 --> 00:27:20,620
into scripts, in this case, Python scripts, and factorize it so that we have one script

318
00:27:20,620 --> 00:27:24,140
doing one step of the pipeline.

319
00:27:24,140 --> 00:27:28,660
So basically, the thing that I often see people needing to do.

320
00:27:28,660 --> 00:27:33,020
So they come in with this Jupyter and say, I want to run it on the cluster.

321
00:27:33,020 --> 00:27:37,820
Well, they can run it on one processor, but that's defeating the purpose.

322
00:27:37,820 --> 00:27:38,820
Yes.

323
00:27:38,820 --> 00:27:39,820
Okay.

324
00:27:39,820 --> 00:27:40,820
Yeah.

325
00:27:40,820 --> 00:27:46,900
When I say efficiently, then yes, I exactly, I meant that we want to use the parallelization

326
00:27:46,900 --> 00:27:47,900
for it.

327
00:27:47,900 --> 00:27:48,900
Yeah.

328
00:27:48,900 --> 00:27:49,900
Okay.

329
00:27:49,900 --> 00:28:01,100
And then, so we take a look at, about kind of like the, where, where we end up with our

330
00:28:01,100 --> 00:28:05,580
scripts and what kind of workflow it is.

331
00:28:05,580 --> 00:28:08,740
It's a simple two-step workflow.

332
00:28:08,740 --> 00:28:18,180
And then we will run that workflow on HPC using, where we parallelize what can be parallelized

333
00:28:18,180 --> 00:28:20,100
using three different methods.

334
00:28:20,100 --> 00:28:26,940
Can you give me a summary of what these methods are just so I'll know what to be looking forward

335
00:28:26,940 --> 00:28:27,940
to?

336
00:28:27,940 --> 00:28:28,940
Okay.

337
00:28:28,940 --> 00:28:40,180
So the first parallelize using scripting is that we basically, we write a Python script

338
00:28:40,180 --> 00:28:44,780
or it can be actually anything, R script also.

339
00:28:44,780 --> 00:28:56,120
But let's say we have a Python script and we use a sub-process module to submit the

340
00:28:56,120 --> 00:28:59,720
job using the Slurm system.

341
00:28:59,720 --> 00:29:09,600
So then those things are running, what is in the loop are running parallel.

342
00:29:09,600 --> 00:29:17,320
And then the array jobs is kind of like a Slurm native method to parallelize.

343
00:29:17,320 --> 00:29:31,880
So we can just modify the Slurm batch script to tell it to that, okay, this needs to be

344
00:29:31,880 --> 00:29:38,000
running in parallel, multiple jobs in parallel.

345
00:29:38,000 --> 00:29:50,520
And the third one is to parallelize or run the whole workflow using a workflow manager

346
00:29:50,520 --> 00:29:51,520
tool.

347
00:29:51,520 --> 00:30:01,840
Which in our case will be SnakeMake.

348
00:30:01,840 --> 00:30:03,800
So what does workflow manager mean?

349
00:30:03,800 --> 00:30:09,760
I've been hearing a lot about how you've been developing the lesson, and I'm really looking

350
00:30:09,760 --> 00:30:13,920
forward to hearing the advantages and disadvantages.

351
00:30:13,920 --> 00:30:31,520
Yeah, it's a workflow manager tool is, we take kind of a more higher view of the workflow.

352
00:30:31,520 --> 00:30:49,480
So we have the whole workflow defined in one file and it is kind of a, when your workflow

353
00:30:49,480 --> 00:30:57,760
is starting to get really big, then basically that is when you might want to switch to using

354
00:30:57,760 --> 00:31:04,400
to workflow managers. But it works for small workflows as well, of course.

355
00:31:04,400 --> 00:31:07,600
Okay, so I look forward to getting there.

356
00:31:07,600 --> 00:31:08,600
Yeah.

357
00:31:08,600 --> 00:31:09,600
So, do we begin?

358
00:31:09,600 --> 00:31:15,360
Yeah, that wasn't a very good explanation, but yeah. You put me on the spot. This is

359
00:31:15,360 --> 00:31:16,360
what you got.

360
00:31:16,360 --> 00:31:22,400
Yeah, okay. So, should I begin with converting it to the Python script?

361
00:31:22,400 --> 00:31:25,800
Yeah, let's start with the notebook.

362
00:31:25,800 --> 00:31:30,160
So I click here.

363
00:31:30,160 --> 00:31:33,440
OK, so now it's time for me to do typing.

364
00:31:33,440 --> 00:31:37,760
So I guess the goal, here I am.

365
00:31:37,760 --> 00:31:41,760
I am on our cluster.

366
00:31:41,760 --> 00:31:45,280
And I'm in my home directory here.

367
00:31:48,360 --> 00:31:52,400
And I'll be doing these examples for almost the first time.

368
00:31:52,400 --> 00:31:55,240
So I will try to go as quick as I can.

369
00:31:55,240 --> 00:31:58,600
But I'll also be asking questions about what it means

370
00:31:58,600 --> 00:31:59,920
and how to do it.

371
00:31:59,920 --> 00:32:02,280
Yeah, please do, because, yeah.

372
00:32:02,280 --> 00:32:06,120
Do we expect users to follow along at the same time?

373
00:32:06,120 --> 00:32:07,080
No.

374
00:32:07,080 --> 00:32:08,000
OK.

375
00:32:08,000 --> 00:32:11,880
Definitely, definitely not, because there

376
00:32:11,880 --> 00:32:15,680
will be enough stuff going on that it will.

377
00:32:15,680 --> 00:32:17,960
Yeah.

378
00:32:17,960 --> 00:32:20,080
So this is just a demo.

379
00:32:20,080 --> 00:32:22,360
Yeah, it's not worth splitting your focus.

380
00:32:22,360 --> 00:32:24,200
Yeah, OK.

381
00:32:24,200 --> 00:32:30,040
So I'll try to explain or ask questions about the parts that might be tricky, but if it's

382
00:32:30,040 --> 00:32:32,760
probably not tricky, I'll just do it as quick as I can.

383
00:32:33,560 --> 00:32:33,800
Okay.

384
00:32:34,440 --> 00:32:34,760
Okay.

385
00:32:34,760 --> 00:32:36,440
But let's see how this goes.

386
00:32:36,440 --> 00:32:36,600
Yeah.

387
00:32:37,480 --> 00:32:41,800
So the notebook is on GitHub.

388
00:32:43,640 --> 00:32:49,160
I guess I can open this and hopefully it renders.

389
00:32:49,160 --> 00:32:50,760
Yeah, looks like a notebook.

390
00:32:50,760 --> 00:32:57,760
But the conversion is already done.

391
00:32:57,760 --> 00:32:58,760
Yes.

392
00:32:58,760 --> 00:33:00,760
Is that the same one?

393
00:33:00,760 --> 00:33:08,760
So it is mainly that we don't have to now start opening notebooks and running them.

394
00:33:08,760 --> 00:33:16,760
So maybe the first thing I will do is make a new directory for my work.

395
00:33:16,760 --> 00:33:28,120
TTT for HPC parallel.

396
00:33:28,120 --> 00:33:40,400
And I will download the script, and I will do that by copying the raw link, and wginted.

397
00:33:40,400 --> 00:33:43,400
Okay.

398
00:33:43,400 --> 00:33:44,400
Okay.

399
00:33:44,400 --> 00:33:45,400
Yeah.

400
00:33:45,400 --> 00:33:50,680
list. I've got it downloaded. Yeah. Okay.

401
00:33:50,680 --> 00:34:03,080
So actually I would, now that I, if we go downwards, there might, it might be that the,

402
00:34:03,080 --> 00:34:12,440
cause, or actually I would prefer that. Let's, let's look a bit like what, what the notebook

403
00:34:12,440 --> 00:34:14,480
actually does or what the code does.

404
00:34:14,480 --> 00:34:15,480
Okay.

405
00:34:15,480 --> 00:34:18,720
Should I open the notebook or the code?

406
00:34:18,720 --> 00:34:24,160
You can open the code because we are now, we have exported it.

407
00:34:24,160 --> 00:34:27,440
So what does it do?

408
00:34:27,440 --> 00:34:32,320
So in this, this is a pipeline or a workflow.

409
00:34:32,320 --> 00:34:36,100
And what we do is that we have two steps.

410
00:34:36,100 --> 00:34:39,120
And in the first step, we...

411
00:34:39,120 --> 00:34:46,800
Or should I be sharing the screen?

412
00:34:46,800 --> 00:34:48,040
Lowering the code.

413
00:34:48,040 --> 00:34:49,040
This is the file.

414
00:34:49,040 --> 00:34:50,040
The script file.

415
00:34:50,040 --> 00:34:51,040
Yes.

416
00:34:51,040 --> 00:34:52,040
Yeah.

417
00:34:52,040 --> 00:34:53,040
That looks...

418
00:34:53,040 --> 00:35:03,560
So this is the notebook converted to a Python script.

419
00:35:03,560 --> 00:35:17,600
So there are two steps here, and the first one is called pre-processing data.

420
00:35:17,600 --> 00:35:24,180
And it uses the Skykit Learn datasets.

421
00:35:24,180 --> 00:35:30,700
It loads this iris flower dataset from there.

422
00:35:30,700 --> 00:35:40,020
So these datasets are something that comes bundled with the Skykit Learn datasets.

423
00:35:40,020 --> 00:35:42,100
So it's automatic, yeah.

424
00:35:42,100 --> 00:35:43,100
Yeah.

425
00:35:43,100 --> 00:35:51,100
And then we take two features from there, which are the length of the sepals of the

426
00:35:51,100 --> 00:35:59,900
flowers. And then we take the classes, which are different subspecies of viruses.

427
00:35:59,900 --> 00:36:09,820
Oops. What? GitHub's popping up Windows. Okay. Yeah. So we have the features and the classes.

428
00:36:10,540 --> 00:36:17,020
Yeah. And then when we do the train, we split the data to train and test sets.

429
00:36:17,020 --> 00:36:22,020
and then we save all this to disk.

430
00:36:23,020 --> 00:36:23,860
Yeah. Okay.

431
00:36:23,860 --> 00:36:28,540
So we have the train and test sets and features and classes.

432
00:36:28,540 --> 00:36:29,540
Yeah.

433
00:36:29,540 --> 00:36:33,420
And it's in the data pre-processed iris pickle file.

434
00:36:34,500 --> 00:36:36,700
And this is, so this was the first step.

435
00:36:36,700 --> 00:36:40,720
And the second step is that we want to,

436
00:36:40,720 --> 00:36:52,240
we want to learn or train these nearest neighbor classifiers or a classifier on this dataset.

437
00:36:53,600 --> 00:37:02,000
And then we apply the learned classifier on the whole data, the complete data,

438
00:37:02,000 --> 00:37:05,840
and plot the decision boundaries.

439
00:37:05,840 --> 00:37:06,680
Okay.

440
00:37:07,520 --> 00:37:08,600
Yeah.

441
00:37:08,600 --> 00:37:10,160
Yeah.

442
00:37:10,160 --> 00:37:13,280
Here we are assuming that that classification

443
00:37:13,280 --> 00:37:18,280
is somewhat familiar topic.

444
00:37:19,800 --> 00:37:20,640
Okay.

445
00:37:20,640 --> 00:37:24,720
So what happens first is we load the preprocessed data

446
00:37:24,720 --> 00:37:29,240
and then we have a couple of parameters

447
00:37:29,240 --> 00:37:37,920
for the classifier, which is the number of neighbors and the distance metric.

448
00:37:37,920 --> 00:37:39,960
Right. Yeah. Okay.

449
00:37:39,960 --> 00:37:40,960
Yeah.

450
00:37:40,960 --> 00:37:46,280
And, spoiler alert, this is what we're going to be parallelizing over.

451
00:37:46,280 --> 00:37:47,280
Yes.

452
00:37:47,280 --> 00:37:48,280
Different.

453
00:37:48,280 --> 00:37:49,280
Yes.

454
00:37:49,280 --> 00:37:50,280
Okay. Yeah.

455
00:37:50,280 --> 00:37:51,280
Exactly.

456
00:37:51,280 --> 00:37:54,280
And here I see two loops.

457
00:37:54,280 --> 00:38:04,800
Yeah. So we have two loops. And what happens is that the, well, like you said, we have

458
00:38:04,800 --> 00:38:18,020
a combination of parameters. And those can be run in parallel, the training and plotting,

459
00:38:18,020 --> 00:38:24,120
because they don't depend on each other.

460
00:38:24,120 --> 00:38:29,960
So we have, OK, this is the training here.

461
00:38:29,960 --> 00:38:31,240
Yes.

462
00:38:31,240 --> 00:38:32,560
And this is the plotting.

463
00:38:32,560 --> 00:38:39,640
And the plotting is just of the respective parameters.

464
00:38:39,640 --> 00:38:43,880
It makes a scatterplot with colors of the classes.

465
00:38:43,880 --> 00:38:46,960
Yeah, so yes.

466
00:38:46,960 --> 00:38:50,360
So important thing here is to, the most important

467
00:38:50,360 --> 00:38:54,280
is that for each parameter settings,

468
00:38:54,280 --> 00:38:58,200
we will have an output file or a result file,

469
00:38:58,200 --> 00:39:03,200
which is, you can see it in the build safe fig.

470
00:39:03,280 --> 00:39:08,280
So in a results folder, we have a image file,

471
00:39:09,720 --> 00:39:14,720
the boundary decisions named according to the parameters,

472
00:39:16,840 --> 00:39:19,200
which is exactly what we were talking about earlier,

473
00:39:19,200 --> 00:39:24,200
that we should always, in order not to write over stuff

474
00:39:27,320 --> 00:39:30,000
and especially not the same time.

475
00:39:30,000 --> 00:39:31,280
Yeah. Okay.

476
00:39:31,280 --> 00:39:34,440
Name the output files.

477
00:39:34,440 --> 00:39:35,280
Good.

478
00:39:36,240 --> 00:39:37,080
Okay.

479
00:39:37,080 --> 00:39:38,080
Now I understand.

480
00:39:38,080 --> 00:39:39,360
So, yeah.

481
00:39:39,360 --> 00:39:41,320
So if we go back,

482
00:39:42,800 --> 00:39:45,600
then because we have, here we have two steps

483
00:39:47,080 --> 00:39:48,920
or we had two steps.

484
00:39:48,920 --> 00:39:53,920
So like we said earlier that we want to factorize the code

485
00:39:55,160 --> 00:39:57,840
so that each step is in its own script.

486
00:39:59,120 --> 00:40:04,120
And here is exactly what we have done in the next section.

487
00:40:05,680 --> 00:40:08,520
So we will have preprocess.py

488
00:40:08,520 --> 00:40:12,240
which will have the first portion of the script.

489
00:40:12,240 --> 00:40:14,520
So here it opens and saves it.

490
00:40:14,520 --> 00:40:18,520
Yes. And the second will be train and plot.

491
00:40:19,120 --> 00:40:27,820
Okay, so here it opens the pre-processed data and iterates over.

492
00:40:29,120 --> 00:40:30,520
Yeah, here the lists are bigger.

493
00:40:31,020 --> 00:40:31,220
Yeah.

494
00:40:31,320 --> 00:40:31,620
Okay.

495
00:40:32,220 --> 00:40:32,520
Yeah.

496
00:40:34,820 --> 00:40:41,620
There are, I think, I think SkyGitLearn has like nine distance metrics that it supports.

497
00:40:41,620 --> 00:40:44,620
So, yeah, we have used five.

498
00:40:44,620 --> 00:40:46,620
Yeah. Okay.

499
00:40:46,620 --> 00:40:51,620
So, number of neighbors goes from 1, 2, 4, 8, 16, 32, 64,

500
00:40:51,620 --> 00:40:55,620
and distance matrix goes Euclidean one and L1 one.

501
00:40:55,620 --> 00:40:56,620
Yeah.

502
00:40:56,620 --> 00:40:58,620
Opposite and cosine.

503
00:40:58,620 --> 00:41:02,620
So, should I copy the files from here or split it myself?

504
00:41:05,620 --> 00:41:07,620
I think you can copy.

505
00:41:07,620 --> 00:41:08,620
Okay.

506
00:41:08,620 --> 00:41:13,620
just to make sure that we don't make typos.

507
00:41:16,540 --> 00:41:17,860
Mess it up somewhere.

508
00:41:18,980 --> 00:41:20,980
So copy, paste.

509
00:41:26,540 --> 00:41:28,860
I hope this pasted correctly.

510
00:41:34,180 --> 00:41:35,320
Just me or there?

511
00:41:35,320 --> 00:41:40,200
There's extra lines in there.

512
00:41:40,200 --> 00:41:41,200
Yeah.

513
00:41:41,200 --> 00:41:48,760
For some reason, it's doubled the line, the empty lines.

514
00:41:48,760 --> 00:41:53,120
Well, let's hope it works.

515
00:41:53,120 --> 00:41:54,120
Interesting.

516
00:41:54,120 --> 00:42:04,200
Is this a weird feature, or I'm not a weird user?

517
00:42:04,200 --> 00:42:10,160
This is actually Emacs, I have the VI program alias to Emacs.

518
00:42:10,160 --> 00:42:12,600
So that makes it even more confusing.

519
00:42:12,600 --> 00:42:13,600
Confusing, yeah.

520
00:42:13,600 --> 00:42:23,000
Okay, well, that's preprocess, and next is train and plot.py.

521
00:42:23,000 --> 00:42:30,960
Did it again.

522
00:42:30,960 --> 00:42:36,120
Okay, so the two files are there.

523
00:42:36,120 --> 00:42:37,120
Yeah.

524
00:42:37,120 --> 00:42:45,200
And then the next section here is the update code to run on a cluster.

525
00:42:45,200 --> 00:42:52,960
And first, we will need the environment with the dependencies.

526
00:42:52,960 --> 00:43:02,880
And that is, we will use this singularity container, or an image, and that has already

527
00:43:02,880 --> 00:43:05,720
been created for us.

528
00:43:05,720 --> 00:43:11,280
So we should be able to, I think you can run this, yeah.

529
00:43:11,280 --> 00:43:13,720
And we can let it run while it's going.

530
00:43:13,720 --> 00:43:15,760
So this is what we talked about last week.

531
00:43:15,760 --> 00:43:16,760
Yeah, exactly.

532
00:43:16,760 --> 00:43:21,100
I did it last night and it worked.

533
00:43:21,100 --> 00:43:32,100
People are already experts with containers, so we don't have to go.

534
00:43:32,100 --> 00:43:42,380
And then what we mean by running in HPC here is, of course, that we want to do the Slurm

535
00:43:42,380 --> 00:43:45,540
submission.

536
00:43:45,540 --> 00:43:50,100
And this is the Slurm script to submit our job.

537
00:43:50,100 --> 00:43:51,100
Yes.

538
00:43:51,100 --> 00:43:52,100
Okay.

539
00:43:52,100 --> 00:43:53,100
So, yeah.

540
00:43:53,100 --> 00:44:09,100
So what it does, it asks some basic resources, time memory, CPUs per task, and then it runs

541
00:44:09,100 --> 00:44:12,220
the two scripts.

542
00:44:12,220 --> 00:44:18,220
And importantly, it runs them sequentially.

543
00:44:18,220 --> 00:44:21,180
So this is doing all 10 or whatever.

544
00:44:21,180 --> 00:44:23,180
Yeah, exactly.

545
00:44:23,180 --> 00:44:24,180
Yeah.

546
00:44:24,180 --> 00:44:30,340
So in the train and plot.py, we currently have the two for loops.

547
00:44:30,340 --> 00:44:40,220
So it will run them all like one by one within the Python script.

548
00:44:40,220 --> 00:44:48,940
In a real thing, would we have the preprocess in the same batch script as the training and

549
00:44:48,940 --> 00:44:51,940
plotting?

550
00:44:51,940 --> 00:45:05,780
I would have different batch scripts for the different scripts, at least for one reason.

551
00:45:05,780 --> 00:45:12,420
There might be more, but at least one reason, which is that different steps require different

552
00:45:12,420 --> 00:45:13,420
resources.

553
00:45:13,420 --> 00:45:14,420
Yeah.

554
00:45:14,420 --> 00:45:20,420
So, it might be that our preprocessing step requires a lot of memory and our training

555
00:45:20,420 --> 00:45:22,220
doesn't for some reason.

556
00:45:22,220 --> 00:45:23,220
Yeah.

557
00:45:23,220 --> 00:45:30,180
So, in this case, does the preprocess script, does it need to be, like, can it be done once

558
00:45:30,180 --> 00:45:32,780
and never done again?

559
00:45:32,780 --> 00:45:33,780
Yes.

560
00:45:33,780 --> 00:45:39,580
Okay, so again, if we were doing multiple analyses, we could have it do it once and

561
00:45:39,580 --> 00:45:43,740
then just leave it at that.

562
00:45:43,740 --> 00:45:44,740
Yes.

563
00:45:44,740 --> 00:45:45,740
Okay.

564
00:45:45,740 --> 00:45:46,740
Yeah.

565
00:45:46,740 --> 00:45:47,740
Yeah.

566
00:45:47,740 --> 00:45:48,740
So should I make a script here?

567
00:45:48,740 --> 00:45:59,420
Yeah, I think, let's see, or should we do a summary of what we have at the moment?

568
00:45:59,420 --> 00:46:00,420
Yeah, sure.

569
00:46:00,420 --> 00:46:05,420
So we have the preprocess.py script, Python script.

570
00:46:05,660 --> 00:46:09,020
We have the train and plot Python script.

571
00:46:09,020 --> 00:46:13,420
And in the train and plot, we do two forward loops.

572
00:46:13,420 --> 00:46:14,260
Okay, yeah.

573
00:46:15,260 --> 00:46:19,300
And then we have a Slurm script

574
00:46:19,300 --> 00:46:22,380
to run the code in like sequentially

575
00:46:22,380 --> 00:46:24,860
without any paralysis.

576
00:46:24,860 --> 00:46:27,300
Using one CPU, it seems.

577
00:46:27,300 --> 00:46:32,580
Yeah. And memory is one giga and time is one hour.

578
00:46:32,580 --> 00:46:33,620
Yeah. Okay.

579
00:46:34,420 --> 00:46:35,060
Okay.

580
00:46:35,060 --> 00:46:36,420
What should I name the script?

581
00:46:39,860 --> 00:46:42,340
I think you can name it...

582
00:46:44,340 --> 00:46:44,980
Submit.sh.

583
00:46:45,700 --> 00:46:47,460
Or submit sequentially.

584
00:46:49,220 --> 00:46:50,420
Non-parallel, maybe.

585
00:46:50,420 --> 00:46:54,740
Right, yeah. It needs to have that in there.

586
00:46:54,740 --> 00:47:01,380
Oh, Nano doesn't have extra lines.

587
00:47:01,380 --> 00:47:02,380
Okay, good.

588
00:47:02,380 --> 00:47:05,500
So now it's all here.

589
00:47:05,500 --> 00:47:07,780
So do we go to the next one now?

590
00:47:07,780 --> 00:47:10,340
Should we look at the notes?

591
00:47:10,340 --> 00:47:12,340
What questions do people have?

592
00:47:12,340 --> 00:47:14,780
Yeah, definitely.

593
00:47:14,780 --> 00:47:16,420
Can you see these also?

594
00:47:16,420 --> 00:47:17,420
There's questions.

595
00:47:17,420 --> 00:47:18,420
SnakeMake, Fireworks...

596
00:47:18,420 --> 00:47:29,160
Yeah, the what is the container used in the example. So, so it's a obtainer or a singularity

597
00:47:29,160 --> 00:47:42,580
container. And, and yeah, it was made in or been created in advance. And it's okay. Yeah.

598
00:47:42,580 --> 00:47:46,060
Yeah, so you made this for this lesson?

599
00:47:46,060 --> 00:47:48,060
Yeah, [name] made it.

600
00:47:48,060 --> 00:47:50,060
I can't take credit.

601
00:47:50,060 --> 00:47:52,060
Yeah, okay.

602
00:47:52,060 --> 00:47:54,060
Okay, good.

603
00:47:56,060 --> 00:47:58,060
Do we need to make a container?

604
00:47:58,060 --> 00:48:00,060
No, it was just easier here.

605
00:48:01,060 --> 00:48:04,060
And actually it is kind of nice here.

606
00:48:04,060 --> 00:48:07,060
At first I thought, oh, a container, do I want to have to use this?

607
00:48:07,060 --> 00:48:11,060
But, you know, I really don't have to do anything else to just one pull command.

608
00:48:11,060 --> 00:48:14,100
So yes, that feels kind of good.

609
00:48:14,100 --> 00:48:15,340
Yes, exactly.

610
00:48:15,340 --> 00:48:16,860
Okay.

611
00:48:16,860 --> 00:48:18,380
I agree.

612
00:48:18,380 --> 00:48:19,220
Yeah.

613
00:48:19,220 --> 00:48:22,020
I was also kind of like, oh man,

614
00:48:22,020 --> 00:48:26,220
do we have to like juggle with images,

615
00:48:26,220 --> 00:48:28,820
but no, just one pool.

616
00:48:28,820 --> 00:48:29,660
Okay.

617
00:48:30,900 --> 00:48:32,660
Should we go on then?

618
00:48:32,660 --> 00:48:33,780
Yes, yes.

619
00:48:33,780 --> 00:48:36,060
So there's seven minutes to the break.

620
00:48:36,060 --> 00:48:38,980
We can parallelize using scripting.

621
00:48:38,980 --> 00:48:40,020
Yeah.

622
00:48:40,020 --> 00:48:45,260
I guess we're running a little bit behind, but that's OK.

623
00:48:45,260 --> 00:48:48,580
So what happens here?

624
00:48:48,580 --> 00:48:54,860
OK, so even if we are using this happily parallel type

625
00:48:54,860 --> 00:49:00,740
of approach, there still needs to be,

626
00:49:00,740 --> 00:49:02,980
we need to make modifications.

627
00:49:02,980 --> 00:49:04,020
OK.

628
00:49:04,020 --> 00:49:14,020
So we can't just like submit stuff and it would automatically work.

629
00:49:14,020 --> 00:49:18,780
And I guess this work is going to be needed for all of the methods anyway.

630
00:49:18,780 --> 00:49:22,360
Yes, yes.

631
00:49:22,360 --> 00:49:33,900
So the point is that if we look at what needs to be done, we have to do the same kind of

632
00:49:33,900 --> 00:49:40,820
work in a form or another.

633
00:49:40,820 --> 00:49:55,060
So basically, I'm wondering where is the, like, do we list that what we are going to

634
00:49:55,060 --> 00:49:58,060
do?

635
00:49:58,060 --> 00:50:02,740
I think it's in the intro.

636
00:50:02,740 --> 00:50:12,660
So basically, because now we have the parameters hard-coded in the script.

637
00:50:12,660 --> 00:50:14,460
So we're removing that.

638
00:50:14,460 --> 00:50:15,460
Yeah.

639
00:50:15,460 --> 00:50:20,260
So yeah, we don't want the parameters to be hard-coded in the script.

640
00:50:20,260 --> 00:50:25,020
We want to give them as command line parameters for the script.

641
00:50:25,020 --> 00:50:29,820
So should I basically make these edits in there?

642
00:50:29,820 --> 00:50:31,820
Yes.

643
00:50:31,820 --> 00:50:35,940
So this is the train and plot one, because that has the loop.

644
00:50:35,940 --> 00:50:36,940
Yes.

645
00:50:36,940 --> 00:50:37,940
Okay.

646
00:50:37,940 --> 00:50:46,940
And how we've got all these extra lines, cool.

647
00:50:46,940 --> 00:50:52,500
So we add, so I'm basically adding in the emphasized lines in here, at least on my screen,

648
00:50:52,500 --> 00:50:58,140
I can see some of these lines have an extra yellow, yellow.

649
00:50:58,140 --> 00:51:08,460
So arg parse is the standard Python command line argument parser.

650
00:51:08,460 --> 00:51:13,980
Okay, so here right above load preprocessed data.

651
00:51:13,980 --> 00:51:17,420
So I copied this and paste.

652
00:51:17,420 --> 00:51:18,420
Yeah.

653
00:51:18,420 --> 00:51:22,180
So, well, I know what this means.

654
00:51:22,180 --> 00:51:27,980
So this is the standard boilerplate for making an argument parser.

655
00:51:27,980 --> 00:51:36,060
giving it one argument, which is the number of neighbors. It's int type, some help text,

656
00:51:36,860 --> 00:51:44,620
and here we're getting n neighbors from the command line. Okay. And for metrics,

657
00:51:45,420 --> 00:51:52,940
I see we've added more metrics. Or not. They are the same metrics, but note that we are now

658
00:51:52,940 --> 00:51:59,820
giving only the number of neighbors as a parameter, and the metrics are still hard-coded as a list.

659
00:52:01,020 --> 00:52:03,580
So when we run...

660
00:52:07,340 --> 00:52:10,460
Yeah, so I just commented it out for clarity.

661
00:52:10,460 --> 00:52:22,540
Yes, yeah, good. So what the script now does is instead of looping over two lists,

662
00:52:22,940 --> 00:52:30,780
of neighbors and metrics. Now it will take the number of neighbors as an argument,

663
00:52:31,420 --> 00:52:35,900
so it's a single value, and then it loops over the number of neighbors.

664
00:52:36,460 --> 00:52:45,340
Okay, so I need to modify down here also. So basically I remove this loop?

665
00:52:45,340 --> 00:53:03,820
Yes. Okay. I want to edit this in Emacs. Sorry. Because I know how to do that. I wish I didn't

666
00:53:03,820 --> 00:53:19,340
have all these extra lines well you can tell this is a real demo yeah this this

667
00:53:19,340 --> 00:53:30,320
was not pre-recorded okay so I remove this loop because now it's done yeah

668
00:53:30,320 --> 00:53:39,360
from command line. And then I need to de-indent all of this, which I can do with Emacs like that.

669
00:53:40,880 --> 00:53:46,800
By the way, did we read the nNeighbors from the args as a variable?

670
00:53:47,840 --> 00:53:48,880
Yeah, it was.

671
00:53:48,880 --> 00:53:49,680
Okay, yes.

672
00:53:49,680 --> 00:53:53,520
Yeah, it's already done there. Okay, so can I save?

673
00:53:54,800 --> 00:53:56,000
I think you can, yeah.

674
00:53:57,600 --> 00:53:58,640
Okay, good.

675
00:53:58,640 --> 00:54:00,200
Good.

676
00:54:00,200 --> 00:54:01,040
Okay.

677
00:54:01,040 --> 00:54:01,860
And then.

678
00:54:04,440 --> 00:54:05,280
Yeah.

679
00:54:07,400 --> 00:54:10,120
Do we need to update the Slurm scripts?

680
00:54:10,120 --> 00:54:10,960
Yeah.

681
00:54:11,880 --> 00:54:12,720
Okay.

682
00:54:12,720 --> 00:54:13,540
Okay.

683
00:54:13,540 --> 00:54:18,320
So the first step was the parameters as comment line

684
00:54:18,320 --> 00:54:19,160
arguments.

685
00:54:24,360 --> 00:54:25,880
Yeah.

686
00:54:25,880 --> 00:54:29,300
I'm making the preprocessing submission script now.

687
00:54:29,300 --> 00:54:30,140
Yeah.

688
00:54:30,140 --> 00:54:31,380
So yeah, this is what I was saying.

689
00:54:31,380 --> 00:54:34,100
So yeah, it can be split into two.

690
00:54:34,100 --> 00:54:35,580
It can be split into two.

691
00:54:35,580 --> 00:54:38,560
And I think here we are requesting

692
00:54:39,940 --> 00:54:41,940
differing amounts of resources.

693
00:54:41,940 --> 00:54:42,980
Oh yeah.

694
00:54:42,980 --> 00:54:45,940
This is 30 minutes and 500 megabytes.

695
00:54:45,940 --> 00:54:46,780
Yeah.

696
00:54:46,780 --> 00:54:49,680
Which I guess is overkill, but that's how it is.

697
00:54:50,620 --> 00:54:52,860
And then what should this one be called?

698
00:54:52,860 --> 00:55:00,860
So, that would be Submit Trainer Plot.

699
00:55:00,860 --> 00:55:08,860
Okay, I put it there.

700
00:55:08,860 --> 00:55:10,860
Yep.

701
00:55:10,860 --> 00:55:12,860
Okay.

702
00:55:12,860 --> 00:55:14,860
Great.

703
00:55:14,860 --> 00:55:16,860
So, make the submission script.

704
00:55:16,860 --> 00:55:18,860
Yeah.

705
00:55:18,860 --> 00:55:20,860
I see.

706
00:55:20,860 --> 00:55:32,220
Yeah, so we could, now we can run the, can you show the submit trainer?

707
00:55:32,220 --> 00:55:44,180
Oh, okay, no, yeah.

708
00:55:44,180 --> 00:55:49,380
Do we have the submit trainer batch script?

709
00:55:49,380 --> 00:55:55,380
Submit trained. Let's see what we have.

710
00:55:57,740 --> 00:56:00,540
This should have been an .sh script.

711
00:56:00,540 --> 00:56:02,460
Yeah, exactly.

712
00:56:02,460 --> 00:56:04,540
Okay.

713
00:56:04,540 --> 00:56:08,620
Can you show it?

714
00:56:09,620 --> 00:56:11,700
Yes.

715
00:56:11,700 --> 00:56:16,940
There is the $1, the number of neighbors.

716
00:56:16,940 --> 00:56:26,440
So, this also takes the first argument given to it in the command line, which is the number

717
00:56:26,440 --> 00:56:27,440
of neighbors.

718
00:56:27,440 --> 00:56:36,940
So, when we do sbatch, we have a Python script here, which for the number of neighbors, submits

719
00:56:36,940 --> 00:56:46,660
a separate batch job with the submission.sh script and with a command line argument, which

720
00:56:46,660 --> 00:56:54,020
get sent via Slurm to the script, and that becomes this $1.

721
00:56:54,020 --> 00:57:09,820
It will not be a Slurm script, but it will, like, it will, yeah, basically what you said

722
00:57:09,820 --> 00:57:18,460
was true, but there was this, that it's actually the Python script that calls the sbat submission

723
00:57:18,460 --> 00:57:26,540
dot sh as a sub process. And then because it's a sub process, it's a command line command,

724
00:57:27,980 --> 00:57:37,020
then the first argument will be the neighbors or I in neighbors. And important thing here is that

725
00:57:37,020 --> 00:57:46,380
the sub process, when it loops over the neighbors, the sub process will not care about the

726
00:57:48,300 --> 00:57:57,340
result. It will not wait for the result. So, it will submit the job and then immediately go to the

727
00:57:57,340 --> 00:58:05,980
next item in the list. Okay. And maybe we can do this when we come back. I just saw a note that

728
00:58:05,980 --> 00:58:15,100
it's time. But there is one good comment in the notes here. Isn't this going to be difficult

729
00:58:15,100 --> 00:58:22,860
to manage if we submit many independent jobs? Yes, exactly. So what that means is that,

730
00:58:22,860 --> 00:58:29,740
let's say that the job for number 16 failed. In order to rerun that, I'd have to go here,

731
00:58:29,740 --> 00:58:35,820
edit this list so it just has number 16, rerun it, watch the output, and repeat it again.

732
00:58:35,980 --> 00:58:45,340
And that's exactly what will be solved with the WorkflowManager method.

733
00:58:45,340 --> 00:58:49,660
And if there were, like, hundreds of these neighbors here, you also wouldn't want to

734
00:58:49,660 --> 00:58:56,760
do this because it's submitting many independent batch jobs, I guess.

735
00:58:56,760 --> 00:59:00,660
And for that, the next step, the ArrayJobs will be better.

736
00:59:00,660 --> 00:59:07,660
Yes, that's also a good point, yeah.

737
00:59:07,660 --> 00:59:10,660
Okay, is there anything else before we go to the break?

738
00:59:10,660 --> 00:59:14,660
There is a...

739
00:59:14,660 --> 00:59:21,660
Maybe we can have a break now and then we can continue later.

740
00:59:21,660 --> 00:59:22,660
Yeah, exactly.

741
00:59:22,660 --> 00:59:25,660
So let's come back at 14 past the hour.

742
00:59:25,660 --> 00:59:27,660
So 10 minutes starting now.

743
00:59:27,660 --> 00:59:28,660
Okay.

744
00:59:28,660 --> 00:59:29,660
That's good.

745
00:59:29,660 --> 00:59:30,500
Bye.

746
00:59:30,500 --> 00:59:31,340
Bye.

747
00:59:33,000 --> 00:59:38,960
Yeah, so instead of running the thing, the idea would be that maybe we can run the scripts

748
00:59:38,960 --> 00:59:42,160
uh, after lunch together.

749
00:59:43,160 --> 00:59:43,400
Yeah.

750
00:59:43,840 --> 00:59:44,080
Yeah.

751
00:59:44,840 --> 00:59:50,560
Cause, cause running will basically just produce the result,

752
00:59:51,040 --> 00:59:52,280
result files and take time.

753
00:59:54,200 --> 00:59:56,480
So, yeah.

754
00:59:56,680 --> 00:59:57,000
Okay.

755
00:59:57,000 --> 01:00:04,000
So, but there is one, uh, uh, what there was this, uh, can you go back to the.

756
01:00:09,360 --> 01:00:16,560
Yeah. So there are, there is this post-processing steps kind of note. So

757
01:00:22,240 --> 01:00:29,920
yeah. So yeah. So this is not, we don't have a post-processing phase here now, but like

758
01:00:29,920 --> 01:00:37,920
this is the, you could basically expand the pipeline or the workflow by the post-processing.

759
01:00:37,920 --> 01:00:40,080
and this section talks about that.

760
01:00:48,960 --> 01:00:53,600
Okay, do you want to take a look at the comments or

761
01:00:59,280 --> 01:01:02,560
the interactive, no, the shared document?

762
01:01:02,560 --> 01:01:15,600
Headstock. That's the one. Are there questions that should... Okay.

763
01:01:15,600 --> 01:01:35,640
Okay, so next we are actually looking at the RA jobs and there is, and if we go to the,

764
01:01:35,640 --> 01:01:41,640
so the last section was parallelized using scripting.

765
01:01:41,640 --> 01:01:50,000
And then where, and then the next one would be parallelize using slurm array job.

766
01:01:50,000 --> 01:01:59,080
So the, because the preprocessing, the preprocess.py, it has to be run only once.

767
01:01:59,080 --> 01:02:03,680
So that situation will not change here.

768
01:02:03,680 --> 01:02:08,760
That section or that script will be identical.

769
01:02:08,760 --> 01:02:24,880
But the parallelization portion could be done with the array jobs.

770
01:02:24,880 --> 01:02:29,120
So what is the array job?

771
01:02:29,120 --> 01:02:54,920
It is a Slurm native way to submit these embarrassingly parallel jobs.

772
01:02:54,920 --> 01:03:08,080
Is the colon to like every other or no, yeah, I don't know if you can tell, but I don't

773
01:03:08,080 --> 01:03:17,660
use array jobs very much because, yeah, and the reason is like related to the question

774
01:03:17,660 --> 01:03:20,920
that is this better or worse, the script.

775
01:03:20,920 --> 01:03:25,920
Sorry, people couldn't hear me. I was muted.

776
01:03:25,920 --> 01:03:26,920
Okay.

777
01:03:26,920 --> 01:03:27,920
Yeah. Okay.

778
01:03:27,920 --> 01:03:35,920
Okay. So, what was I saying? This relates to the question in the HedgeDoc.

779
01:03:35,920 --> 01:03:43,920
Which is preferable? Array jobs or the scripting?

780
01:03:43,920 --> 01:03:59,520
And there I would say that both are like viable options, but there are like different advantages

781
01:03:59,520 --> 01:04:01,240
here.

782
01:04:01,240 --> 01:04:10,200
And the main advantage of using Array Jobs is that because it's learn native, you can

783
01:04:10,200 --> 01:04:18,000
add in the sbatch options, you can add, for example, email notification that you get a

784
01:04:18,000 --> 01:04:26,840
notification in email when the job has been submitted and when the job is done, either

785
01:04:26,840 --> 01:04:32,920
completed or failed.

786
01:04:32,920 --> 01:04:42,680
That is one advantage of Array Jobs, but the scripts have one advantage that I really like

787
01:04:42,680 --> 01:04:51,720
is that it's really easy to navigate with different kinds of parameters.

788
01:04:51,720 --> 01:05:00,240
Because you can give the command line parameters that are eventually given to looped over and

789
01:05:00,240 --> 01:05:03,920
given to the Python script here.

790
01:05:03,920 --> 01:05:08,920
They can be like integers,

791
01:05:09,320 --> 01:05:10,760
like the number of neighbors,

792
01:05:10,760 --> 01:05:13,920
or they can be strings, like the distance metrics,

793
01:05:13,920 --> 01:05:16,100
or they can be floats and they can be anything,

794
01:05:16,100 --> 01:05:18,300
and it's easy to navigate with them.

795
01:05:20,620 --> 01:05:25,300
On contrast or in contrast with the array jobs,

796
01:05:27,380 --> 01:05:29,220
what we do with the array jobs

797
01:05:29,220 --> 01:05:34,220
is that we give this kind of array of indices,

798
01:05:36,520 --> 01:05:41,520
which can then be accessed with SLURM_ARRAY_TASK_ID.

799
01:05:42,560 --> 01:05:45,280
And that this gets mapped.

800
01:05:45,280 --> 01:05:49,280
Yes, and there is the, yeah, exactly.

801
01:05:49,280 --> 01:05:54,280
So of course, these are like one, two, three, four, five,

802
01:05:54,320 --> 01:05:57,080
or it can be two, from two to a hundred,

803
01:05:57,080 --> 01:06:02,460
And no, it's not divided by two, but like, okay.

804
01:06:02,460 --> 01:06:03,120
But okay.

805
01:06:03,120 --> 01:06:03,620
Anyways.

806
01:06:04,100 --> 01:06:05,900
So anyways, there are integers.

807
01:06:07,180 --> 01:06:12,580
So if you want to use, if you want to use strings, then you'd need to have this

808
01:06:12,620 --> 01:06:14,980
kind of extra mapping step there.

809
01:06:17,540 --> 01:06:22,420
And so, so there are both.

810
01:06:22,420 --> 01:06:32,300
So, the point is both techniques are viable and valid, but they have, it's a little bit

811
01:06:32,300 --> 01:06:34,300
of a different flavor.

812
01:06:34,300 --> 01:06:35,300
Yeah.

813
01:06:35,300 --> 01:06:36,300
Okay.

814
01:06:36,300 --> 01:06:47,540
So, what, do we do this or is this an exercise?

815
01:06:47,540 --> 01:06:48,540
It is an exercise.

816
01:06:48,540 --> 01:06:49,540
Okay.

817
01:06:49,540 --> 01:06:54,260
This is something users can do themselves, or learners can do themselves.

818
01:06:54,260 --> 01:06:55,260
Yeah.

819
01:06:55,260 --> 01:06:56,260
Okay.

820
01:06:56,260 --> 01:07:01,260
And actually, by the way, there is an exercise in the Parallelize using scripting as well.

821
01:07:01,260 --> 01:07:02,260
Okay.

822
01:07:02,260 --> 01:07:05,260
So this is what we can do after lunch.

823
01:07:05,260 --> 01:07:08,060
Yeah, exactly.

824
01:07:08,060 --> 01:07:16,940
And the exercise there is that instead of looping over the number of neighbors in the

825
01:07:16,940 --> 01:07:24,140
script, we loop over both number of neighbors and the distance

826
01:07:24,140 --> 01:07:28,940
metrics, which is exactly this case that we have different

827
01:07:28,940 --> 01:07:34,460
kinds of parameters. And so with the scripting, it's really

828
01:07:34,460 --> 01:07:40,260
easy to kind of modify your code a little bit. So it takes the

829
01:07:40,260 --> 01:07:44,420
arguments in and loop it over.

830
01:07:44,420 --> 01:07:47,540
Yeah, okay.

831
01:07:47,940 --> 01:07:49,780
Do we go on then?

832
01:07:49,780 --> 01:07:51,380
Uh, yes.

833
01:07:51,380 --> 01:07:52,900
Okay.

834
01:07:52,900 --> 01:07:56,020
So now to the workflow manager one.

835
01:07:56,020 --> 01:07:56,740
Yeah.

836
01:07:56,740 --> 01:07:58,740
And this is where I think it will start

837
01:07:58,740 --> 01:08:00,260
to get interesting,

838
01:08:00,260 --> 01:08:02,900
or at least I hope.

839
01:08:02,900 --> 01:08:04,580
Uh, yeah.

840
01:08:04,580 --> 01:08:06,420
Yeah, because the scripting and

841
01:08:06,420 --> 01:08:08,820
ArrayJobs are very, they are kind of

842
01:08:08,820 --> 01:08:26,820
like the very straightforward ways, or as straightforward as this stuff can be.

843
01:08:26,820 --> 01:08:32,980
Yeah, like it's basically using typical shell scripting in different ways to make loops,

844
01:08:32,980 --> 01:08:35,540
while the workflow manager is really something new.

845
01:08:35,540 --> 01:08:50,580
Yeah, exactly. And I think actually the motivation part here in the workflow manager is quite...

846
01:08:50,580 --> 01:08:54,580
What's the word? Well, complete.

847
01:08:54,580 --> 01:08:56,620
Oops. Okay. Yeah.

848
01:08:56,620 --> 01:09:06,620
So again, to recap, we have the preprocess.py and train-and-plot.py, which are our computational

849
01:09:06,620 --> 01:09:16,620
steps here. And the preprocess.py is run first.

850
01:09:16,620 --> 01:09:21,860
It only needs to be run once.

851
01:09:21,860 --> 01:09:24,180
And needs to be run only once.

852
01:09:24,180 --> 01:09:25,180
Yeah.

853
01:09:25,180 --> 01:09:37,980
So you can, if you move your cursor a bit slower, and then after that, when the preprocess has run,

854
01:09:39,100 --> 01:09:46,460
we have the dataset on the disk, and then we can submit multiple jobs of train and plot

855
01:09:47,180 --> 01:09:52,940
using different number of neighbors and distance metrics values.

856
01:09:52,940 --> 01:09:59,860
And now I see why you made a separate preprocess.py script, because I guess you'll tell SnakeMake

857
01:09:59,860 --> 01:10:08,580
to run this once and run it separately, and it's somehow all...

858
01:10:08,580 --> 01:10:16,220
I think already with the previous scripts, we wanted to have them separately.

859
01:10:16,220 --> 01:10:22,820
Because otherwise, if the pre-processing stuff is in the train and plot script, or they are

860
01:10:22,820 --> 01:10:38,900
in the same script, then for every kind of job or parameter pair we want to run, we will

861
01:10:38,900 --> 01:10:42,200
always do the pre-processing as well.

862
01:10:42,200 --> 01:10:50,560
And that is not necessary and that is just wasting resources because we need to do the

863
01:10:50,560 --> 01:10:52,200
dataset only once.

864
01:10:52,200 --> 01:10:53,200
Yeah.

865
01:10:53,200 --> 01:10:54,200
Okay.

866
01:10:54,200 --> 01:10:55,200
So.

867
01:10:55,200 --> 01:10:56,200
Okay.

868
01:10:56,200 --> 01:11:06,600
So, there is this kind of note that the submission scripts and array jobs work well for this

869
01:11:06,600 --> 01:11:11,160
kind of small workflows and are usually the go-to solution because they are kind of like

870
01:11:11,160 --> 01:11:19,400
low threshold. But however, if the workflow, if our workflow would be larger, so let's

871
01:11:19,400 --> 01:11:27,560
say we have like multiple pre-processing steps, we can have multiple processing steps. So

872
01:11:27,560 --> 01:11:35,120
instead of just one train and plot, we have like multiple training scripts and then we

873
01:11:35,120 --> 01:11:40,880
We have, and then we have post-processing scripts.

874
01:11:40,880 --> 01:11:48,560
So where we gather the results and maybe do a kind of like a summary of the results.

875
01:11:48,560 --> 01:11:57,520
So in that case, the workflow gets larger and more complicated.

876
01:11:57,520 --> 01:12:06,120
And it is, in that case, it may be a situation where we want to have this workflow manager

877
01:12:06,120 --> 01:12:07,120
tool.

878
01:12:07,120 --> 01:12:08,120
Yeah.

879
01:12:08,120 --> 01:12:09,120
Okay.

880
01:12:09,120 --> 01:12:19,440
Although, again, a reminder that you can also use workflow manager for small workflows,

881
01:12:19,440 --> 01:12:23,800
but there's nothing wrong with that.

882
01:12:23,800 --> 01:12:27,200
Yeah, okay.

883
01:12:27,200 --> 01:12:33,800
So what is a workflow manager?

884
01:12:33,800 --> 01:12:39,880
The workflow manager is, this is how I would, this is how I think of it.

885
01:12:39,880 --> 01:12:55,840
So it's a tool where we define the workflow using rules.

886
01:12:55,840 --> 01:13:04,180
And each rule is one of our computational steps.

887
01:13:04,180 --> 01:13:10,420
So pre-processing or pre-process or training plot in our case.

888
01:13:10,420 --> 01:13:20,060
And each of those rules or steps take as an input file or files.

889
01:13:20,060 --> 01:13:35,900
their output is a file or files. So then you have this kind of like a graph where we create

890
01:13:37,660 --> 01:13:44,780
one rule creates a file and then another rule takes that file as input and creates another file

891
01:13:44,780 --> 01:13:47,740
and another step takes that as an input then.

892
01:13:47,740 --> 01:13:52,300
So I guess it's like this concept, you don't say how to do it, you say

893
01:13:52,300 --> 01:13:57,980
what needs to be done, and then let the workflow manager figure out

894
01:13:57,980 --> 01:14:03,020
what's already done, what's not already done, the best order to do it.

895
01:14:03,020 --> 01:14:07,260
It knows how to do stuff at the same time if it doesn't depend on each other.

896
01:14:07,260 --> 01:14:08,780
Yeah, exactly.

897
01:14:08,780 --> 01:14:12,620
Like the declarative programming language versus the

898
01:14:12,620 --> 01:14:17,980
imperative or something like that. Yeah, exactly. Okay. So, so basically you,

899
01:14:18,780 --> 01:14:30,780
and also just to reiterate the idea that we, we do these rules that are the steps and then,

900
01:14:31,340 --> 01:14:37,980
then we have a kind of like a final rule that are the, what are the, what are the final

901
01:14:37,980 --> 01:14:45,380
target files that we want to have after this workflow.

902
01:14:45,380 --> 01:14:56,480
And so when you say that these are the final files that I want to ultimately end up with,

903
01:14:56,480 --> 01:15:03,540
then you can go step back and step back and step back to the beginning and that the workflow

904
01:15:03,540 --> 01:15:06,460
miniature does this automatically.

905
01:15:06,460 --> 01:15:07,260
Yeah, OK.

906
01:15:09,940 --> 01:15:11,220
OK, so.

907
01:15:14,900 --> 01:15:18,700
OK, so there are three points there, so.

908
01:15:18,700 --> 01:15:25,180
So the order, if it already exists, and then doesn't.

909
01:15:25,180 --> 01:15:27,900
Or order and parallelization.

910
01:15:27,900 --> 01:15:29,620
Yeah, so it's, yeah.

911
01:15:29,620 --> 01:15:30,120
Yeah.

912
01:15:30,120 --> 01:15:37,240
So it looks for the order, looks in the past for what should be done in the future.

913
01:15:37,240 --> 01:15:38,960
Well, anyway, I think we've said this enough.

914
01:15:38,960 --> 01:15:40,520
Yeah, yeah, exactly.

915
01:15:40,520 --> 01:15:43,720
Should we start doing it?

916
01:15:43,720 --> 01:15:48,400
Yeah, but one more thing about that.

917
01:15:48,400 --> 01:15:57,120
Here we are using SnakeMake and SnakeMake is very popular,

918
01:15:57,120 --> 01:16:02,120
especially in the, is it bioinformatics field?

919
01:16:02,120 --> 01:16:03,400
Okay.

920
01:16:03,400 --> 01:16:08,400
And it's a, it's written in Python,

921
01:16:10,360 --> 01:16:13,760
but what happens in the computational steps

922
01:16:13,760 --> 01:16:15,920
is like they can be anything.

923
01:16:15,920 --> 01:16:17,360
They don't have to be Python.

924
01:16:18,800 --> 01:16:22,120
And the workflow script that we're gonna see

925
01:16:22,120 --> 01:16:24,520
in just a moment is also,

926
01:16:24,520 --> 01:16:28,000
It's basically Python.

927
01:16:28,000 --> 01:16:30,320
It's a flavor of Python.

928
01:16:30,320 --> 01:16:34,360
So it's a Python-like scripting language.

929
01:16:34,360 --> 01:16:35,360
Yeah.

930
01:16:35,360 --> 01:16:36,360
Okay.

931
01:16:36,360 --> 01:16:43,880
And there are hundreds of workflow miniature tools, but SnakeMake is what we use here.

932
01:16:43,880 --> 01:16:49,520
Do you know why we chose SnakeMake?

933
01:16:49,520 --> 01:17:00,320
Well, people in our proximity have experience with it.

934
01:17:00,320 --> 01:17:01,720
It is well known.

935
01:17:01,720 --> 01:17:07,920
And also because I use Python, I do everything with Python.

936
01:17:07,920 --> 01:17:14,360
So it's kind of like a lowest threshold of, or what is it, barrier of entry?

937
01:17:14,360 --> 01:17:15,360
Yeah.

938
01:17:15,360 --> 01:17:16,360
It's a common thing.

939
01:17:16,360 --> 01:17:26,680
I didn't know the Code Refinery lessons teach it, but I wonder how that was done.

940
01:17:26,680 --> 01:17:27,680
Anyway.

941
01:17:27,680 --> 01:17:33,120
Oh, but that's a good point, because this is Code Refinery as well.

942
01:17:33,120 --> 01:17:37,480
So in this Code Refinery context, when we're talking about workflow manager, we're probably

943
01:17:37,480 --> 01:17:39,480
talking about SnakeMake.

944
01:17:39,480 --> 01:17:40,480
Yeah.

945
01:17:40,480 --> 01:17:41,480
Okay.

946
01:17:41,480 --> 01:17:42,480
Okay.

947
01:17:42,480 --> 01:17:45,960
So, to access SnakeMake.

948
01:17:45,960 --> 01:17:51,960
Yeah, we still don't get to the actual workflow.

949
01:17:51,960 --> 01:18:00,880
This is like a common theme with workflow manager, that there is a lot of tinkering.

950
01:18:00,880 --> 01:18:09,360
And the first question is, okay, we have this Python package, SnakeMake, how do I actually

951
01:18:09,360 --> 01:18:12,960
access it on an HPC cluster.

952
01:18:12,960 --> 01:18:19,880
And if you're on your own laptop and you want to develop locally, then you can just use

953
01:18:19,880 --> 01:18:25,720
BIP to install the SnakeMake.

954
01:18:25,720 --> 01:18:35,720
But then on the HPC clusters, on many clusters, the situation is that you cannot install your

955
01:18:35,720 --> 01:18:49,860
own packages or software. Like, for example, on Triton, Aalto Triton, you can make, create

956
01:18:49,860 --> 01:18:59,260
conda environments. But, for example, on Lumi, there isn't that kind of option.

957
01:18:59,260 --> 01:19:04,900
But since I'm in Aalto, should I do the module load?

958
01:19:04,900 --> 01:19:18,300
Yeah, exactly. So the important thing here is that it is up to the cluster administration

959
01:19:18,300 --> 01:19:27,500
to provide a way to use Snakemake and similar tools.

960
01:19:27,500 --> 01:19:34,580
If someone lets you, can you do pip install stake make yourself on your own cluster?

961
01:19:34,580 --> 01:19:39,500
Yes, if the admins let you.

962
01:19:39,500 --> 01:19:40,500
Yeah.

963
01:19:40,500 --> 01:19:41,500
Okay.

964
01:19:41,500 --> 01:19:42,500
Yeah.

965
01:19:42,500 --> 01:19:43,500
Okay.

966
01:19:43,500 --> 01:19:44,500
So I will come back here.

967
01:19:44,500 --> 01:19:47,820
I can make this smaller.

968
01:19:47,820 --> 01:19:56,180
So there are two examples there, that on CSC Puhti and Aalto Triton, they both have,

969
01:19:56,180 --> 01:20:04,140
CSC Puhti has its own SnakeMake documentation, how to use SnakeMake on Puhti.

970
01:20:04,140 --> 01:20:13,220
And on Aalto Triton, if we do the module load Skype, this generic scientific computing Python

971
01:20:13,220 --> 01:20:16,020
environment, then we will get that.

972
01:20:16,020 --> 01:20:17,020
Oh, okay.

973
01:20:17,020 --> 01:20:18,020
You already did it.

974
01:20:18,020 --> 01:20:19,020
Yeah.

975
01:20:19,020 --> 01:20:20,020
Okay.

976
01:20:20,020 --> 01:20:28,020
So you can actually try it out, like if you write snake make and enter them, it should.

977
01:20:28,020 --> 01:20:33,020
Oh, okay.

978
01:20:33,020 --> 01:20:34,020
There's some.

979
01:20:34,020 --> 01:20:35,020
Okay.

980
01:20:35,020 --> 01:20:40,020
It definitely, it definitely like.

981
01:20:40,020 --> 01:20:45,020
Why is it accessing the license server?

982
01:20:45,020 --> 01:20:49,020
I don't know.

983
01:20:49,020 --> 01:20:52,020
This is something that is very...

984
01:20:53,020 --> 01:20:54,020
Let me try it as well.

985
01:20:54,020 --> 01:20:56,020
It's definitely that.

986
01:20:57,020 --> 01:21:01,020
Let's just hope this doesn't come up when we actually try to run the thing.

987
01:21:01,020 --> 01:21:02,020
Yeah, exactly.

988
01:21:02,020 --> 01:21:07,020
I guess there's not some random snake file here.

989
01:21:09,020 --> 01:21:10,020
Huh.

990
01:21:11,020 --> 01:21:16,020
Maybe it's trying to activate all of these different plugins and...

991
01:21:19,020 --> 01:21:20,020
Yeah.

992
01:21:20,020 --> 01:21:21,020
Yeah.

993
01:21:21,020 --> 01:21:22,020
Oh, okay.

994
01:21:22,020 --> 01:21:33,420
Well, if you run, yeah, if you run snake make help, then it will, it will print less of

995
01:21:33,420 --> 01:21:34,420
that stuff.

996
01:21:34,420 --> 01:21:35,420
Okay.

997
01:21:35,420 --> 01:21:36,420
Yeah.

998
01:21:36,420 --> 01:21:37,420
Okay.

999
01:21:37,420 --> 01:21:38,420
There's a little bit less.

1000
01:21:38,420 --> 01:21:40,100
So we have access to snake make.

1001
01:21:40,100 --> 01:21:41,100
Yeah.

1002
01:21:41,100 --> 01:21:42,100
Okay.

1003
01:21:42,100 --> 01:21:43,100
Hmm.

1004
01:21:43,100 --> 01:21:44,100
That was interesting.

1005
01:21:44,100 --> 01:21:45,100
That was, yeah.

1006
01:21:45,100 --> 01:21:48,100
That, that gave me a little bit of, uh, anxiety.

1007
01:21:48,100 --> 01:21:49,100
Anxiety.

1008
01:21:49,100 --> 01:21:50,100
Okay.

1009
01:21:50,100 --> 01:21:51,100
So let's see.

1010
01:21:51,100 --> 01:21:52,100
Yeah.

1011
01:21:52,100 --> 01:21:55,260
Let's see if the if it works at all then.

1012
01:21:55,260 --> 01:21:56,260
Yeah.

1013
01:21:56,260 --> 01:21:57,260
Let's make the workflow.

1014
01:21:57,260 --> 01:22:00,460
So do I make a new file called snake file?

1015
01:22:00,460 --> 01:22:01,460
Yes.

1016
01:22:01,460 --> 01:22:07,500
I guess that's a play on make file.

1017
01:22:07,500 --> 01:22:08,500
Yes.

1018
01:22:08,500 --> 01:22:15,500
And try this is copy all of this into it.

1019
01:22:15,500 --> 01:22:16,500
Yeah.

1020
01:22:16,500 --> 01:22:33,620
So what we need is two files in the end, which is the snake file, which is the file that

1021
01:22:33,620 --> 01:22:35,820
describes and defines the workflow.

1022
01:22:35,820 --> 01:22:36,820
Yeah.

1023
01:22:36,820 --> 01:22:40,100
So I see metrics, neighbors list, yeah.

1024
01:22:40,100 --> 01:22:45,140
And then we need the kind of configuration, aka profile file.

1025
01:22:45,140 --> 01:22:46,140
That's here.

1026
01:22:46,140 --> 01:22:47,140
Yeah.

1027
01:22:47,140 --> 01:22:50,820
And we will check the profile file after this snake file.

1028
01:22:50,820 --> 01:22:51,820
Yeah.

1029
01:22:51,820 --> 01:22:52,820
Okay.

1030
01:22:52,820 --> 01:23:02,420
So, do you want to use the Emacs or like, let's look at what is in there in the...

1031
01:23:02,420 --> 01:23:03,420
In which one?

1032
01:23:03,420 --> 01:23:04,420
Snakefile?

1033
01:23:04,420 --> 01:23:05,420
Snakefile.

1034
01:23:05,420 --> 01:23:06,420
Okay.

1035
01:23:06,420 --> 01:23:11,220
Well, we could look here, it's colorized.

1036
01:23:11,220 --> 01:23:19,500
Yeah, it took so long to get there, so let's look at it.

1037
01:23:19,500 --> 01:23:29,700
So what we have here is that first we have the parameter values.

1038
01:23:29,700 --> 01:23:33,300
So this looks familiar.

1039
01:23:33,300 --> 01:23:34,780
Yes.

1040
01:23:34,780 --> 01:23:42,480
And here is the rule all, which we already mentioned.

1041
01:23:42,480 --> 01:23:55,040
So we have one rule, which is conventionally called rule all, that actually lists all the

1042
01:23:55,040 --> 01:24:02,820
output files that we are like ultimately interested in, that when the workflow is done, then these

1043
01:24:02,820 --> 01:24:04,900
These are the files that we want to have.

1044
01:24:04,900 --> 01:24:05,900
Okay.

1045
01:24:05,900 --> 01:24:06,900
Yeah.

1046
01:24:06,900 --> 01:24:10,980
So it's sort of like pooling in all of these, which are the outputs, which will be defined

1047
01:24:10,980 --> 01:24:11,980
below.

1048
01:24:11,980 --> 01:24:12,980
Yeah.

1049
01:24:12,980 --> 01:24:13,980
Okay.

1050
01:24:13,980 --> 01:24:24,740
And then here is an example of like, that, like, that this is, it looks a bit like Python,

1051
01:24:24,740 --> 01:24:28,420
but it's not Python per se.

1052
01:24:28,420 --> 01:24:40,220
And for example, this expand is one of those snake makes own like syntaxes that it's basically

1053
01:24:40,220 --> 01:24:46,020
it does a for loop or two for loops in this case.

1054
01:24:46,020 --> 01:24:55,420
So for each neighbors and metric, it creates this and it's a list in the end, what comes

1055
01:24:55,420 --> 01:25:09,420
out of the expand. And then the next rule is we're kind of like going from the end to

1056
01:25:09,420 --> 01:25:21,180
the beginning. So the next rule is train and plot. And what it does, it reads in the data

1057
01:25:21,180 --> 01:25:34,220
pre-processed iris pickle. And it will output the results slash neighbors metric image file.

1058
01:25:35,260 --> 01:25:40,060
So output, yeah, this is output PNG with the respective stuff.

1059
01:25:41,260 --> 01:25:48,620
And this is how it connects to the rule all because this rule's output is the same as

1060
01:25:48,620 --> 01:25:54,380
rule all's input. So this is the same file name. Yeah, exactly. And I guess these end

1061
01:25:54,380 --> 01:26:04,860
neighbors variables come automatically from here somehow. Yes, yeah, exactly. The snake make will

1062
01:26:05,820 --> 01:26:17,260
kind of, it will inject the values from place to another. Yeah.

1063
01:26:18,620 --> 01:26:28,620
And then one nice thing about this workflow is that we can just give it the container.

1064
01:26:28,620 --> 01:26:37,860
So in our case, the Apptainer container, and what it does, it says to SnakeMake that, okay,

1065
01:26:37,860 --> 01:26:47,020
we want this rule to be run in this container, which is really nice because the container

1066
01:26:47,020 --> 01:26:53,180
can be different for each room. So if you have computational steps that need to be run

1067
01:26:53,180 --> 01:27:00,260
in very different environments, that's no problem. You can make an image or container

1068
01:27:00,260 --> 01:27:04,100
image for each of those environments.

1069
01:27:04,100 --> 01:27:08,740
And shell is just the command to actually run inside the container, I guess. So it has

1070
01:27:08,740 --> 01:27:09,740
Exactly.

1071
01:27:09,740 --> 01:27:17,740
Wildcards metric. What's `wildcards.metric`?

1072
01:27:17,740 --> 01:27:35,740
Yeah, so the wildcards is again, snake make syntax. And for some reason, when you actually do the shell, or you run the command,

1073
01:27:35,740 --> 01:27:43,620
And you want to refer to the parameter.

1074
01:27:43,620 --> 01:27:46,180
You need to use these wildcards, don't.

1075
01:27:46,180 --> 01:27:47,180
Okay.

1076
01:27:47,180 --> 01:27:48,180
Yeah.

1077
01:27:48,180 --> 01:27:49,180
I'll just accept that.

1078
01:27:49,180 --> 01:27:50,180
Yeah.

1079
01:27:50,180 --> 01:27:52,540
As you can see in the output, you don't have to.

1080
01:27:52,540 --> 01:27:57,140
In the log file, you don't have to, but in the shell, you have to.

1081
01:27:57,140 --> 01:27:58,140
Okay.

1082
01:27:58,140 --> 01:27:59,140
Why?

1083
01:27:59,140 --> 01:28:00,140
Who knows?

1084
01:28:00,140 --> 01:28:01,140
Yeah.

1085
01:28:01,140 --> 01:28:02,140
Okay.

1086
01:28:02,140 --> 01:28:05,180
Next, I guess, is the preprocessing rule.

1087
01:28:05,180 --> 01:28:08,580
And it's basically, from what I've seen above,

1088
01:28:08,580 --> 01:28:09,900
I can tell this is the same.

1089
01:28:09,900 --> 01:28:14,900
So there's output file, which is input to this rule,

1090
01:28:15,060 --> 01:28:19,340
container log, and the shell command.

1091
01:28:19,340 --> 01:28:20,540
Yeah.

1092
01:28:20,540 --> 01:28:21,980
Okay.

1093
01:28:21,980 --> 01:28:25,140
And the container is the same, but it could be different.

1094
01:28:25,140 --> 01:28:26,740
Yeah.

1095
01:28:26,740 --> 01:28:27,580
Okay.

1096
01:28:27,580 --> 01:28:31,340
And how they are connected is that, again,

1097
01:28:31,340 --> 01:28:37,020
input of the rule train and plot is the output of the rule pre-process.

1098
01:28:38,620 --> 01:28:44,220
Yes. Okay. For the snake make profile file.

1099
01:28:44,220 --> 01:28:51,260
Yeah. So what is the snake make profile file? So profile file is basically the configuration

1100
01:28:51,260 --> 01:28:59,420
file, and it can have a lot of configuration, but in our case, the most important thing is that

1101
01:28:59,420 --> 01:29:12,220
in this profile file, we will tell SnakeMake that when it submits a Slurm job,

1102
01:29:14,140 --> 01:29:25,420
what resources to request. And the nice thing here again, is that we can, if you see the,

1103
01:29:25,420 --> 01:29:34,100
the, well, let's go from the top. So we tell the executor is learn. So if we would have

1104
01:29:34,100 --> 01:29:41,860
a different kind of cluster, it could be something else. And the maximum number of parallel jobs

1105
01:29:41,860 --> 01:29:49,940
is 10, which is good to have. It's good to have like a safety net that if you mess something

1106
01:29:49,940 --> 01:29:57,940
up, then you don't spam the cluster with tens of thousands of jobs.

1107
01:29:57,940 --> 01:30:02,940
Okay. I bet that's happened before.

1108
01:30:02,940 --> 01:30:04,940
Yeah.

1109
01:30:04,940 --> 01:30:06,940
Yeah.

1110
01:30:06,940 --> 01:30:08,940
Yeah.

1111
01:30:08,940 --> 01:30:10,940
And then...

1112
01:30:10,940 --> 01:30:12,940
So, threats...

1113
01:30:12,940 --> 01:30:25,380
Yeah, this is, so I wrote there that in Snakemake threads is equal to CPUs per task, because

1114
01:30:25,380 --> 01:30:34,740
there is this kind of like, because the executor can be anything.

1115
01:30:34,740 --> 01:30:36,620
I guess it's not just for Slurm.

1116
01:30:36,620 --> 01:30:37,620
So yeah, exactly.

1117
01:30:37,620 --> 01:30:38,620
More generic variables.

1118
01:30:38,620 --> 01:30:39,620
Exactly.

1119
01:30:39,620 --> 01:30:40,620
Yeah.

1120
01:30:40,620 --> 01:30:49,740
And the nice thing about, if you check the set threads, the really nice thing about is

1121
01:30:49,740 --> 01:30:56,340
that you can see the rule names, preprocess and train and plot.

1122
01:30:56,340 --> 01:31:03,420
So here we can set different resources for different rules.

1123
01:31:03,420 --> 01:31:15,700
So again, to throw back to the, what was it, the parallelization using scripts, then to

1124
01:31:15,700 --> 01:31:26,700
throw back there, there we have for each script, we have the sub slurm batch script.

1125
01:31:26,700 --> 01:31:34,520
And in that Slurm batch script, we have these resources defined.

1126
01:31:34,520 --> 01:31:39,780
So here we have defined the same resources, 500 megabytes of memory and 30 minutes of

1127
01:31:39,780 --> 01:31:54,300
runtime and one gigabyte of memory and one and two CPUs per task.

1128
01:31:54,300 --> 01:32:00,420
And one more thing, these profile files are configuration files.

1129
01:32:00,420 --> 01:32:11,220
So they can like, you can have like default profile file that the snake make looks in

1130
01:32:11,220 --> 01:32:16,100
your home folder and that kind of stuff.

1131
01:32:16,100 --> 01:32:22,020
So yeah.

1132
01:32:22,020 --> 01:32:26,780
But here we only have this one profile file.

1133
01:32:26,780 --> 01:32:33,980
And finally, the run comment, or do you have anything to, sorry, I'm kind of like pushing

1134
01:32:33,980 --> 01:32:34,980
this.

1135
01:32:34,980 --> 01:32:35,980
Yeah.

1136
01:32:35,980 --> 01:32:39,980
What should I do?

1137
01:32:39,980 --> 01:32:40,980
Yeah.

1138
01:32:40,980 --> 01:32:45,020
So do you have anything to comment or ask about?

1139
01:32:45,020 --> 01:32:47,880
Where do I make the profile file?

1140
01:32:47,880 --> 01:32:50,180
So it is where you put it.

1141
01:32:50,180 --> 01:32:51,180
Yeah.

1142
01:32:51,180 --> 01:32:52,180
Where do I put it?

1143
01:32:52,180 --> 01:32:53,180
Okay.

1144
01:32:53,180 --> 01:32:56,060
So you can see it in the run snakemake command.

1145
01:32:56,060 --> 01:32:57,060
Here?

1146
01:32:57,060 --> 01:32:58,060
Yeah.

1147
01:32:58,060 --> 01:33:07,720
So we will put it in this, we will make a dir folder profiles.

1148
01:33:07,720 --> 01:33:13,880
And inside that we have the slurm.

1149
01:33:13,880 --> 01:33:15,280
So this is a file.

1150
01:33:15,280 --> 01:33:18,160
No, that is also a folder.

1151
01:33:18,160 --> 01:33:19,160
Okay.

1152
01:33:19,160 --> 01:33:20,160
Yeah.

1153
01:33:20,160 --> 01:33:26,400
This is a kind of a confusing thing about the snake make profile in my opinion.

1154
01:33:26,400 --> 01:33:27,400
Okay.

1155
01:33:27,400 --> 01:33:28,400
Yeah.

1156
01:33:28,400 --> 01:33:31,400
Because you don't give the profile file that you want.

1157
01:33:31,400 --> 01:33:37,440
You give a folder which contains the profile file.

1158
01:33:37,440 --> 01:33:38,920
And what should the file name be?

1159
01:33:38,920 --> 01:33:44,920
It's config.yaml with an A.

1160
01:33:44,920 --> 01:33:45,920
Okay.

1161
01:33:45,920 --> 01:33:46,920
Yeah.

1162
01:33:46,920 --> 01:33:49,920
It's also important that it has the A.

1163
01:33:49,920 --> 01:33:50,920
Okay.

1164
01:33:50,920 --> 01:33:53,920
So, I will paste this here.

1165
01:33:53,920 --> 01:33:54,920
Yep.

1166
01:33:54,920 --> 01:33:55,920
Save.

1167
01:33:55,920 --> 01:33:56,920
Exit.

1168
01:33:56,920 --> 01:33:57,920
Okay.

1169
01:33:57,920 --> 01:33:58,920
Okay.

1170
01:33:58,920 --> 01:34:01,920
Should I try running it?

1171
01:34:01,920 --> 01:34:02,920
Yeah, let's try.

1172
01:34:02,920 --> 01:34:05,920
Do you think it works the first time?

1173
01:34:05,920 --> 01:34:07,920
I hope so.

1174
01:34:07,920 --> 01:34:13,440
But, yeah, anything can happen.

1175
01:34:13,440 --> 01:34:22,000
And also, if it works, and even if it works, the output will be really verbose.

1176
01:34:22,000 --> 01:34:26,400
So you might want to, like, make the window bigger.

1177
01:34:26,400 --> 01:34:27,400
Ah, okay.

1178
01:34:27,400 --> 01:34:31,400
Well, we can let it go.

1179
01:34:31,400 --> 01:34:32,400
Okay.

1180
01:34:32,400 --> 01:34:33,400
What?

1181
01:34:33,400 --> 01:34:34,400
Okay.

1182
01:34:34,400 --> 01:34:40,400
Okay, now. Okay. Yeah. Oh, what is this? I wrote password.

1183
01:34:48,400 --> 01:34:55,400
I'm trying to set an environment variable and nope. Nope. I haven't encountered this.

1184
01:34:55,400 --> 01:35:08,960
So, someone in our chat had wondered, do I, like, is it some other configuration that

1185
01:35:08,960 --> 01:35:13,240
I have, but I don't know where it would be?

1186
01:35:13,240 --> 01:35:15,240
Yeah.

1187
01:35:15,240 --> 01:35:44,840
There's no snake make variables, it's just that that I said.

1188
01:35:44,840 --> 01:35:51,840
What could be different in our, well, anything can be different in our configurations, I

1189
01:35:51,840 --> 01:35:52,840
guess.

1190
01:35:52,840 --> 01:36:03,640
Try reloading the environment.

1191
01:36:03,640 --> 01:36:12,720
So we have another cluster admin here in the background, fixing stuff as I'm doing it.

1192
01:36:12,720 --> 01:36:21,540
thing. And I've looked in my home directory here in another window and I

1193
01:36:21,540 --> 01:36:29,880
don't see anything obvious that looks like that looks like a snake make

1194
01:36:29,880 --> 01:36:48,800
make a config file, snake make, can I tell it to not load whatever these other plugins

1195
01:36:48,800 --> 01:36:49,800
I'm here.

1196
01:36:49,800 --> 01:36:50,800
Oh, okay.

1197
01:36:50,800 --> 01:36:51,800
Yeah.

1198
01:36:51,800 --> 01:36:52,800
Yeah.

1199
01:36:52,800 --> 01:37:07,800
It's trying to load some storage plugin, which I don't know why, because so storage plug-in

1200
01:37:07,800 --> 01:37:16,160
meaning that, cause Snakemake may can also incorporate like S3 and those kind of, I would, I would

1201
01:37:16,160 --> 01:37:30,560
thinking. Let's see SciBuilder, make storage plugins. But let's, let's give it like one

1202
01:37:30,560 --> 01:37:38,160
minute and then, and then let's do a wrap up because this was in a way, this was a perfect

1203
01:37:38,160 --> 01:37:50,180
ending for the SnakeMake session. Because this reflects very, very, very well my experience

1204
01:37:50,180 --> 01:37:51,180
with SnakeMake.

1205
01:37:51,180 --> 01:37:54,800
Do I have any Python packages?

1206
01:37:54,800 --> 01:38:02,360
It's a useful tool. This is also something that for those who would like to try the SnakeMake

1207
01:38:02,360 --> 01:38:09,160
after the after lunch in one hour basically you can join the zoom and we can we can try to yeah

1208
01:38:09,160 --> 01:38:16,680
exactly related to the questions in the notes document um someone's asking docker image or

1209
01:38:16,680 --> 01:38:23,320
should be obtainer well the the short answer is that apptainer is the one that will work on a share

1210
01:38:23,320 --> 01:38:28,920
system like an hpc system but of course in your in your computer you could you could do it with

1211
01:38:28,920 --> 01:38:37,080
docker for example because docker often if not always requires this super user administration

1212
01:38:37,080 --> 01:38:41,880
right administrator rights and then snake make will generate the slurm script in a

1213
01:38:41,880 --> 01:38:47,960
correct form automatically yeah that's the idea of these plugins if your system is not slurm

1214
01:38:48,680 --> 01:38:54,360
do i need some additional custom slurm parameters well i would say that if your system is not slurm

1215
01:38:54,360 --> 01:39:00,920
but you're still going to basically use something, I don't know what could be, maybe some Kubernetes

1216
01:39:00,920 --> 01:39:06,680
or some other type of cluster. At the end of the day, you can basically tell what would be the

1217
01:39:06,680 --> 01:39:13,560
executable for each of the processes that SnakeMake runs. So in theory, even on your own laptop,

1218
01:39:13,560 --> 01:39:21,800
if you have multiple CPUs, you can benefit of the parallelization with the multiple CPUs that

1219
01:39:21,800 --> 01:39:26,480
you have in your laptop. But in general, I mean, we're showing the SLURM way of doing

1220
01:39:26,480 --> 01:39:33,240
it just because most of them, if not all of the clusters that we support and we work with

1221
01:39:33,240 --> 01:39:41,240
are basically SLURM based clusters. But it's sometime for wrapping up. We have five minutes

1222
01:39:41,240 --> 01:39:50,960
What's left, wasn't that there was some conclusion, take on messages section in the materials?

1223
01:39:50,960 --> 01:39:54,800
Let me check.

1224
01:39:54,800 --> 01:39:59,720
So what's the conclusions?

1225
01:39:59,720 --> 01:40:08,600
I would like first a 30 second conclusion of this workflow miniatures, say, so what

1226
01:40:08,600 --> 01:40:19,200
What is the advantage of using a workflow manager compared to...

1227
01:40:19,200 --> 01:40:27,080
So there is definitely an advantage, especially if you have a complex workflow, that you kind

1228
01:40:27,080 --> 01:40:34,680
of like the workflow manager will take care of the order of execution and that everything

1229
01:40:34,680 --> 01:40:44,200
is. And if something is already existing, it won't run it again. And also because we

1230
01:40:44,200 --> 01:40:52,360
are using containers and environments, like integrated into the workflow definition, then

1231
01:40:52,360 --> 01:41:03,160
it promotes this reproducibility idea. And the disadvantages is that it might be very,

1232
01:41:03,160 --> 01:41:08,920
It might be hard to get even started because your cluster may not have SnakeMake installed

1233
01:41:08,920 --> 01:41:15,600
and you have to talk to your administration about it, cluster administration.

1234
01:41:15,600 --> 01:41:22,260
And the workflow managers have their own syntaxes for the scripting and they have their own

1235
01:41:22,260 --> 01:41:31,420
practices and ecosystems and you need to spend time to learn and get into them.

1236
01:41:31,420 --> 01:41:37,220
So yeah, there's this learning overhead.

1237
01:41:37,220 --> 01:41:48,960
Like I remember [name] saying when preparing for this, today's lesson, that this is going

1238
01:41:48,960 --> 01:41:56,460
to be very like, what was it, something about SnakeMake being so hard and having so many

1239
01:41:56,460 --> 01:42:04,500
problems when trying to use it and what do you know like here we had a problem it looks

1240
01:42:04,500 --> 01:42:11,700
really cool like once we get all these dependencies set up if I had hundreds of different jobs

1241
01:42:11,700 --> 01:42:19,780
it would definitely be saving me lots of time yeah yeah and for me the workflow actually

1242
01:42:19,780 --> 01:42:25,220
when I when I finally got it working yeah it looks really nice you have snake file and

1243
01:42:25,220 --> 01:42:33,140
have profile file and yeah, it just produces the results. But of course it didn't. Now a different

1244
01:42:33,140 --> 01:42:40,500
user uses it and there's something in the environment that doesn't work. And yeah,

1245
01:42:41,860 --> 01:42:48,740
because I tried it this morning and it worked for me. So I don't know. Okay. So yeah, that was the

1246
01:42:48,740 --> 01:42:53,740
the workflow manager comment from me.

1247
01:42:54,180 --> 01:42:57,020
Sorry, I took three minutes instead of 30 seconds.

1248
01:42:57,020 --> 01:42:58,420
Yeah.

1249
01:42:58,420 --> 01:42:59,420
Okay.

1250
01:42:59,420 --> 01:43:04,420
So can we wrap up with a couple of take home messages?

1251
01:43:04,740 --> 01:43:09,380
And I think you are sharing.

1252
01:43:11,920 --> 01:43:15,460
This is the overall conclusions talk here.

1253
01:43:15,460 --> 01:43:16,300
Yeah.

1254
01:43:18,740 --> 01:43:30,540
So, [name], what should be the take-home message from this day and where can we use them in?

1255
01:43:30,540 --> 01:43:40,580
Use very, very simple workflows so that the debugging is easy because there will be problems

1256
01:43:40,580 --> 01:43:45,580
when doing parallelization and the simpler your workflows,

1257
01:43:46,860 --> 01:43:50,180
then you have a chance of debugging them.

1258
01:43:54,260 --> 01:43:55,460
That would be mine.

1259
01:43:58,060 --> 01:43:59,060
Take home message.

1260
01:43:59,060 --> 01:44:00,380
Take home message.

1261
01:44:00,380 --> 01:44:02,540
And in general, I mean, the things we have covered

1262
01:44:02,540 --> 01:44:06,220
that especially at this concurrent IO,

1263
01:44:06,220 --> 01:44:10,420
if you clearly feel that the parallelization

1264
01:44:10,420 --> 01:44:17,460
seems actually to be slower, then maybe it's worth investigating is it an I.O. issue rather

1265
01:44:17,460 --> 01:44:23,580
than a computing issue. And so at the end of the day, there's many things that can go

1266
01:44:23,580 --> 01:44:30,740
wrong with heavy parallelizations, but it's good to, you know, usually visit your administrator's

1267
01:44:30,740 --> 01:44:38,660
help desk and figure out together where would be the good compromise. So I hope this was

1268
01:44:38,660 --> 01:44:47,700
a useful two hours overview on our various kind of recipes for parallelizing your code.

1269
01:44:49,860 --> 01:44:54,740
In one hour we will have the zoom session so that those who are interested they can actually try

1270
01:44:54,740 --> 01:45:00,100
this on their clusters and we will be there together in the zoom to help you trying these

1271
01:45:00,100 --> 01:45:07,060
examples and for those who need the credit please join the zoom session and let me know that you're

1272
01:45:07,060 --> 01:45:15,620
there so that I can mark your presence. I think this concludes our pilot series of TTT4HPC.

1273
01:45:16,580 --> 01:45:21,300
Thank you everyone who has been developing the materials and many people who you actually have

1274
01:45:21,300 --> 01:45:27,940
not seen in the in the streams during these four days they have helped so much and we will try to

1275
01:45:27,940 --> 01:45:37,220
document all the contributors and managers, whatever. There are also many, many helpers

1276
01:45:37,220 --> 01:45:43,540
that have been in building this series that basically took kind of almost two years from

1277
01:45:43,540 --> 01:45:50,500
its conception to the actual implementation. This was a pilot run, so we will really need

1278
01:45:50,500 --> 01:45:58,260
your feedback in the hack in the notes document there's already a simple feedback form that you

1279
01:45:58,260 --> 01:46:05,700
can answer using this share notes and later I will try to send them send a form like a

1280
01:46:05,700 --> 01:46:13,060
like a questionnaire so that in general if you can tell us how to improve things for the first run in

1281
01:46:13,060 --> 01:46:17,800
in the next fall, that would be great.

1282
01:46:17,800 --> 01:46:22,000
So, [name], do you have any last recommendation words?

1283
01:46:24,720 --> 01:46:25,560
Really?

1284
01:46:25,560 --> 01:46:27,080
Did you say something about credits?

1285
01:46:27,080 --> 01:46:29,240
Are we offering credits for this?

1286
01:46:29,240 --> 01:46:30,800
Yeah, so for those who need the credit,

1287
01:46:30,800 --> 01:46:33,000
please join the Zoom in one hour,

1288
01:46:33,000 --> 01:46:38,000
and in the end, exercises that you need to,

1289
01:46:38,040 --> 01:46:40,360
like, don't worry about the exercises.

1290
01:46:40,360 --> 01:46:42,600
It's more important that you try things out

1291
01:46:42,600 --> 01:46:45,040
and you're able to get them working.

1292
01:46:47,760 --> 01:46:50,320
All right, thank you [name] and thank you [name].

1293
01:46:50,320 --> 01:46:51,160
Thank you.

1294
01:46:51,160 --> 01:46:52,740
And thank you everyone for watching

1295
01:46:52,740 --> 01:46:56,440
and see you soon for our next streamings

1296
01:46:56,440 --> 01:46:58,840
in the CodeRefinery Twitch channel.

1297
01:46:58,840 --> 01:47:00,480
Next adventure.

1298
01:47:00,480 --> 01:47:01,720
Exactly.

1299
01:47:01,720 --> 01:47:02,640
Bye bye.

1300
01:47:02,640 --> 01:47:03,480
Thank you.

1301
01:47:03,480 --> 01:47:04,320
Bye.

